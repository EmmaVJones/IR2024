---
title: "How to run the IR2024 automated assessment"
author: "Emma Jones"
date: "September 2, 2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

options(digits = 12) #critical to seeing all the data

library(tidyverse)
library(sf)
library(readxl)
library(pins)
library(config)
library(EnvStats)
library(lubridate)
library(round) # for correct round to even logic

# Bring in Assessment functions from app
source('automatedAssessmentFunctions.R')
source('updatedBacteriaCriteria.R')

# get configuration settings
conn <- config::get("connectionSettings")

# use API key to register board
board_register_rsconnect(key = conn$CONNECT_API_KEY,  #Sys.getenv("CONNECT_API_KEY"),
                          server = conn$CONNECT_SERVER)#Sys.getenv("CONNECT_SERVER"))

```

This document walks users through running the automated assessement for any given set of stations and waterbody type (riverine or lacustrine for now). Prior to this point, the user needs to have completed the necessary prerequisites of attributing stations with AU and WQS information. This dataset is a companion to the rivers and streams/lake assessment applications and is **NOT** final. The results cited in this table are identical to the app calculations and are meant for assessor review, QA, editing prior to sumbitting to WQA CEDS bulk data upload.

# Prerequisites
The user will have to have all conventionals data, water column metals, and sediment metals organized by Roger for the window. **Additionally, the CEDS EDAS data needs to be exported and available for use, including regional Bioassessment calls**. Last cycle's finalized regional assessment layer (shapefile with AUs) are the spatial component needed. Lastly, the assessor must have the stations bulk data upload table filled out to their requirements in order to run the assessment scripts. 

## Pin Data/Retrieve Data

Scripts to organize and pin data are in ../2.organizeMetadata/organizeStationMetadata_DRAFT.Rmd 

## Bring in Pinned data

Draft data created for testing

```{r pull pins}
conventionals <- pin_get('ejones/conventionals2024draft', board = 'rsconnect')
conventionals_distinct <- pin_get('ejones/conventionals2024_distinctdraft', board = 'rsconnect') %>% 
  filter(!is.na(Latitude) | !is.na(Longitude))


WQMstationFull <- pin_get("ejones/WQM-Station-Full", board = "rsconnect")
VSCIresults <- pin_get("ejones/VSCIresults", board = "rsconnect") %>%
  filter( between(`Collection Date`, assessmentPeriod[1], assessmentPeriod[2]) )
VCPMI63results <- pin_get("ejones/VCPMI63results", board = "rsconnect") %>%
  filter( between(`Collection Date`, assessmentPeriod[1], assessmentPeriod[2]) )
VCPMI65results <- pin_get("ejones/VCPMI65results", board = "rsconnect") %>%
  filter( between(`Collection Date`, assessmentPeriod[1], assessmentPeriod[2]) ) 
assessmentWindowLowFlows <- pin_get('ejones/AssessmentWindowLowFlows', board = 'rsconnect')
```

Placeholders until data window sorted

```{r placeholders}
## Placeholders
stations2020IR <- pin_get("stations2020IR-sf-final", board = "rsconnect")
WCmetals <- pin_get("WCmetals-2022IRfinal",  board = "rsconnect")
Smetals <- pin_get("Smetals-2022IRfinal",  board = "rsconnect")

markPCB <- read_excel('data/oldData/2022 IR PCBDatapull_EVJ.xlsx', sheet = '2022IR Datapull EVJ')
fishPCB <- read_excel('data/oldData/FishTissuePCBsMetals_EVJ.xlsx', sheet= 'PCBs')
fishMetals <- read_excel('data/oldData/FishTissuePCBsMetals_EVJ.xlsx', sheet= 'Metals')
```


## Bring in Station Table from two previous assessments 

```{r stations table 2018}
stationsTable2022 <- readRDS('../2.organizeMetadata/data/stationsTable2022.RDS')
stationsTable2020 <- readRDS('../2.organizeMetadata/data/stationsTable2020.RDS')
```


## Bring in lake nutrient standards

```{r lake nutrient standards}
lakeNutStandards <- read_csv('data/9VAC25-260-187lakeNutrientStandards.csv')
```



# Workflow

The following steps complete the automated assessment.

## Bring in user station table data

This information communicates to the scripts which stations should be assessed and where they should be organized (AUs). It also has data from the last cycle to populate the historical station information table in the application.

**To Note: The AU assignments in this table are valid as of the start of the assessment process. For any AU splits/reassignments, the assessors control that through local (.csv) copies of the output of this script that is uploaded to the relevant Assessment Application on the Connect Server (and subsequently uploaded to CEDS via the bulk data upload tool).**

```{r stationTable, echo = FALSE}
stationTable <- pin_get('ejones/stationsTable2024begin', board = 'rsconnect')
```

## Bring in Station Table Bulk Upload Template

```{r bulk upload template}
stationsTemplate <- stationTable[0,] %>% 
  mutate(across(matches(c("EXC", "SAMP")), as.numeric)) %>% 
  # new addition that breaks station table upload template but very helpful for assessors
  mutate(BACTERIADECISION = as.character(NA),
         BACTERIASTATS = as.character(NA),
         `Date Last Sampled` = as.character(NA)) 

```

## Attach WQS information


Pull pinned WQS info saved on server. Then perform a series of data manipulation steps that:

1. Join WQS to station table by StationID
2. Create a new variable that will correct for Class II pH differences for Tidal Waters
3. Join actual WQS critera (object name WQSvalues) to each StationID
4. Perform a little data cleanup to lose columns unnecessary for future steps
5. Join station ecoregion information (for benthic analyses)
6. Standardize lake names  (to match 187 lake names for future joining of criteria)
    + Some problematic stations need manual steps to attach the correct lake name
7. Join lake nutrient criteria to individual stations
    + Lake Drummond requires some extra manual work per hh special standard https://law.lis.virginia.gov/admincode/title9/agency25/chapter260/section310/
8. Rearrange data schema to more useful format after all previous joins
9. Correct units for nutrient criteria to match conventionals data format (ug/L to mg/L)


```{r pull WQS info}
WQSlookup <- pin_get("WQSlookup-withStandards",  board = "rsconnect")

stationTable <- left_join(stationTable, WQSlookup, by = c('STATION_ID' = 'StationID')) %>% # (1)
  # Fix for Class II Tidal Waters in Chesapeake (bc complicated DO/temp/etc standard)
  mutate(CLASS_BASIN = paste(CLASS,substr(BASIN, 1,1), sep="_")) %>% # (2)
  mutate(CLASS_BASIN = ifelse(CLASS_BASIN == 'II_7', "II_7", as.character(CLASS))) %>% # (2)
  # Join actual WQS criteria to each StationID
  left_join(WQSvalues, by = 'CLASS_BASIN') %>% # (3)
  # data cleanup
  dplyr::select(-c(CLASS.y,CLASS_BASIN)) %>% # (4)
  rename('CLASS' = 'CLASS.x') %>% # (4)
  # Join station ecoregion information (for benthic analyses)
  left_join(dplyr::select(WQMstationFull, WQM_STA_ID, EPA_ECO_US_L3CODE, EPA_ECO_US_L3NAME) %>% #(5)
              distinct(WQM_STA_ID, .keep_all = TRUE), by = c('STATION_ID' = 'WQM_STA_ID')) %>%
  # Standardize lake names (to match 187 lake names for future joining of criteria)
  lakeNameStandardization() %>% # standardize lake names (6)
  
   
  # extra special step (6)
  mutate(Lake_Name = case_when(STATION_ID %in% c('2-TRH000.40') ~ 'Thrashers Creek Reservoir',
                               STATION_ID %in% c('2-LSL000.16') ~ 'Lone Star Lake F (Crystal Lake)',
                               STATION_ID %in% c('2-LSL000.04') ~ 'Lone Star Lake G (Crane Lake)',
                               STATION_ID %in% c('2-LSL000.20') ~ 'Lone Star Lake I (Butler Lake)',
                               STATION_ID %in% c('2-NWB002.93','2-NWB004.67',
                                                 '2-NWB006.06') ~ 'Western Branch Reservoir',
                               STATION_ID %in% c('2-LDJ000.60') ~ 'Lake Nottoway (Lee Lake)',
                               TRUE ~ as.character(Lake_Name))) %>%
  
  # special step for 187 lakes missing designation
  #mutate(Lakes_187B = case_when(STATION_ID == '1BNTH043.48' ~ 'y',
  #                              TRUE ~ as.character(Lakes_187B))) %>% 

  # Join lake nutrient criteria to individual stations
  left_join(lakeNutStandards %>% 
              mutate(Lakes_187B = 'y'),  # special step to make sure the WQS designation for 187 are correct even when not
            by = c('Lake_Name')) %>% # (7)
  # lake drummond special standards
  mutate(Lakes_187B = ifelse(is.na(Lakes_187B.y ), Lakes_187B.x, Lakes_187B.y), 
         # dd. For Lake Drummond, located within the boundaries of Chesapeake and Suffolk in the Great Dismal Swamp, chlorophyll a shall not exceed 35 µg/L and total phosphorus shall not exceed 40 µg/L at a depth of one meter or less. https://law.lis.virginia.gov/admincode/title9/agency25/chapter260/section310/
         `Chlorophyll a (ug/L)` = case_when(Lake_Name %in% c('Lake Drummond') ~ 35,
                                            TRUE ~ as.numeric(`Chlorophyll a (ug/L)`)),
         `Total Phosphorus (ug/L)` = case_when(Lake_Name %in% c('Lake Drummond') ~ 40,
                                               TRUE ~ as.numeric(`Total Phosphorus (ug/L)`))) %>% # (7)
  dplyr::select(STATION_ID:StreamType, Lakes_187B, `Description Of Waters`:`Total Phosphorus (ug/L)`) %>% # (8)
  # match lake limit to TP data unit
  mutate(`Total Phosphorus (mg/L)` = `Total Phosphorus (ug/L)` / 1000) # (9)


# Identify stations that are missing WQS
missingWQS <- filter(stationTable, is.na(CLASS))
```

## Bring in Low Flow Information

We also need to bring in the previously analyzed low flow periods for the assessment window. These USGS stream gages are analyzed using the 7Q10 methodology consistent with the DEQ Water Permitting Program with two assumption changes:

1. Provisional data are accepted (due to the expedited assessment timeline). QA/QC changes from Provision to Accepted data generally involve storm event corrections, so using Provisional data should not affect low flow analyses.
2. Gages analyzed for low flow statistics require 10 or more valid water years for inclusion in analyses.

This gage-specific information is spatially joined to VAHU5 watersheds to extrapolate low flow conditions to stations that fall into said watersheds. The temporal windows where low flow events were recorded in a watershed are joined to stations that fall into said watershed. Parameters that should not be analyzed during low flow events are flagged during the automated assessment process and presented to the assessment staff in the assessment applications.

```{r low flow}
conventionals <- conventionals %>% 
  mutate(SampleDate = as.Date(FDT_DATE_TIME)) %>% 
  left_join(dplyr::select(assessmentWindowLowFlows, Date, `7Q10 Flag`, VAHU5),
            by = c('SampleDate' = 'Date', 'Huc6_Vahu5' = 'VAHU5')) %>% 
  dplyr::select(-SampleDate) # rm unnecessary column

View(filter(conventionals, `7Q10 Flag` == "7Q10 Flag"))
```



## Identify Station Type

We only want to run riverine methods on riverine stations and only lake methods on lake stations, so identify all stations that have lake AU designations. We will use this information later to determine how to analyze certain parameters that have different methods based on waterbody type. 

```{r identify lake stations}
lakeStations <- filter_at(stationTable,
                          .vars = vars(contains("ID305B")),
                          .vars_predicate = any_vars(str_detect(., 'L_')))

# QA check: make sure all 187 lakes have chla and tp standards
# lakeStationsImportant <- dplyr::select(lakeStations, STATION_ID:ID305B_3, NUT_TP_EXC:NUT_CHLA_STAT, WQS_ID, GNIS_Name, WATER_NAME, Lakes_187B, Lake_Name:`Total Phosphorus (ug/L)`) %>%
#   filter(!is.na(WQS_ID))


```

<!-- And drop estuarine stations as well. -->

<!-- ```{r} -->
<!-- estuarineStations <- filter_at(stationTable, -->
<!--                           .vars = vars(contains("ID305B")), -->
<!--                           .vars_predicate = any_vars(str_detect(., 'E_'))) -->
<!-- bothTypesE <- filter_at(estuarineStations, -->
<!--                           .vars = vars(contains("ID305B")), -->
<!--                           .vars_predicate = any_vars(str_detect(., 'R_')))  -->
<!-- #View(dplyr::select(stationTable, STATION_ID, ID305B_1, WQS_ID, everything())) -->

<!-- # filter out estuarine stations for analysis -->
<!-- # stationTable <- filter(stationTable, ! STATION_ID %in% estuarineStations$STATION_ID) %>% -->
<!-- #   bind_rows(bothTypesE) # add northern station back in to be safe -->


<!-- # for running only a few select sites -->
<!-- # stationTable1 <- stationTable -->
<!-- # stationTable <- filter(stationTable, str_detect(STATION_ID, '7-POQ' )) -->
<!-- ``` -->



# Automated Assessment Steps

Finally, we are ready to analyze some data!  Go through conventionals one station at a time, join stationTable, analyze by functions, report in WQA CEDS template


```{r Automated Assessment}
# make placeholder objects to store data results
stationTableResults <- stationsTemplate 
# save ammonia results (based on default assessment information) for use in app to speed rendering
ammoniaAnalysis <- tibble()


# time it:
startTime <- Sys.time()

# loop over all sites, not super efficient but get the job done in easy to follow format
for(i in 15:25){#nrow(stationTable)){
#i = 1
  print(paste('Assessing station', i, 'of', nrow(stationTable), sep=' '))

  # pull one station data
  stationData <- filter(conventionals, FDT_STA_ID %in% stationTable$STATION_ID[i]) %>%
    #stationData <- filter(conventionals, FDT_STA_ID == "4ACRV006.19") %>% #"1ABIR000.76") %>%# test a specific station
    left_join(stationTable, by = c('FDT_STA_ID' = 'STATION_ID')) %>%
    # Special Standards Correction step. This is done on the actual data bc some special standards have temporal components
    pHSpecialStandardsCorrection() %>% # correct pH to special standards where necessary
    temperatureSpecialStandardsCorrection() %>% # correct temperature special standards where necessary
    # special lake steps
    {if(stationTable$STATION_ID[i] %in% lakeStations$STATION_ID)
      suppressWarnings(suppressMessages(
        mutate(., lakeStation = TRUE) %>%
        thermoclineDepth())) # adds thermocline information and SampleDate
      else mutate(., lakeStation = FALSE) }

  # Previous station table comments
  comments <- stationTableComments(stationTable$STATION_ID[i], stationsTable2022, '2022', stationsTable2020, '2020')
  
  # Date last sampled
  if(nrow(stationData) > 0){
    dateLastSampled <- as.character(max(stationData$FDT_DATE_TIME)) } else {dateLastSampled <- 'No data in conventionals data pull'}
  
  # Ammonia special section
  ammoniaAnalysisStation <- freshwaterNH3limit(stationData, trout = ifelse(unique(stationData$CLASS) %in% c('V','VI'), TRUE, FALSE),
                                        mussels = TRUE, earlyLife = TRUE) 
  # https://law.lis.virginia.gov/admincode/title9/agency25/chapter260/section155/ states the assumption is that
  #  waters are to be assessed with the assumption that mussels and early life stages of fish should be present
  # trout presence is determined by WQS class, this can be changed in the app but is forced to be what the station
  # is attributed to in the automated assessment scripts
  
  # PWS stuff
  if(nrow(stationData) > 0){
    if(is.na(unique(stationData$PWS))  ){
      PWSconcat <- tibble(#STATION_ID = unique(stationData$FDT_STA_ID),
                          PWS= NA)
   } else {
     PWSconcat <- cbind(#tibble(STATION_ID = unique(stationData$FDT_STA_ID)),
                         assessPWS(stationData, NITRATE_mg_L, LEVEL_NITRATE, 10, 'PWS_Nitrate'),
                         assessPWS(stationData, CHLORIDE_mg_L, LEVEL_CHLORIDE, 250, 'PWS_Chloride'),
                         assessPWS(stationData, SULFATE_TOTAL_mg_L, LEVEL_SULFATE_TOTAL, 250, 'PWS_Total_Sulfate')) %>%
       dplyr::select(-ends_with('exceedanceRate')) }
    
  # chloride assessment if data exists
    if(nrow(filter(stationData, !is.na(CHLORIDE_mg_L))) > 0){
      chlorideFreshwater <- chlorideFreshwaterSummary(suppressMessages(chlorideFreshwaterAnalysis(stationData)))
    } else {chlorideFreshwater <- tibble(CHL_EXC = NA, CHL_STAT= NA)}
    
  # Water toxics combination with PWS, Chloride Freshwater, and water column PCB data
  if(nrow(bind_cols(PWSconcat,
                    chlorideFreshwater,
                    PCBmetalsDataExists(filter(markPCB, str_detect(SampleMedia, 'Water')) %>%
                                        filter(StationID %in% stationData$FDT_STA_ID), 'WAT_TOX')) %>%
          dplyr::select(contains(c('_EXC','_STAT'))) %>%
          mutate(across( everything(),  as.character)) %>%
          pivot_longer(cols = contains(c('_EXC','_STAT')), names_to = 'parameter', values_to = 'values', values_drop_na = TRUE) %>% 
          filter(! str_detect(values, 'WQS info missing from analysis'))  ) > 1) {
    WCtoxics <- tibble(WAT_TOX_EXC = NA, WAT_TOX_STAT = 'Review')
    } else { WCtoxics <- tibble(WAT_TOX_EXC = NA, WAT_TOX_STAT = NA)} 
  
  } else {
    WCtoxics <- tibble(WAT_TOX_EXC = NA, WAT_TOX_STAT = NA)
  }
   
  # If data exists for station, run it, otherwise just output what last cycle said and comment
  if(nrow(stationData) > 0){
    # Nutrients based on station type
    # Nutrient: TP (lakes have real standards; riverine no longer uses 0.2 mg/L as an observed effect for Aquatic life use)
      # NUT_TP_EXC, NUT_TP_SAMP, NUT_TP_STAT
    if(unique(stationData$lakeStation) == TRUE){
      TP <- TP_Assessment(stationData) 
        #tibble(NUT_TP_EXC = NA, NUT_TP_SAMP = NA, NUT_TP_STAT = NA) # placeholder for now
      } else {
        TP <- countNutrients(stationData, PHOSPHORUS_mg_L, LEVEL_PHOSPHORUS, NA) %>% quickStats('NUT_TP') %>% 
          mutate(NUT_TP_STAT = ifelse(NUT_TP_STAT != "S", "Review", NA)) } # flag OE but don't show a real assessment decision
    
      # Nutrients: Chl a (lakes)
    if(unique(stationData$lakeStation) == TRUE){
      chla <- chlA_Assessment(stationData)
        #tibble(NUT_CHLA_EXC = NA, NUT_CHLA_SAMP = NA, NUT_CHLA_STAT = NA) # placeholder for now
      } else {
        chla <- countNutrients(stationData, CHLOROPHYLL_A_ug_L, LEVEL_CHLOROPHYLL_A, NA) %>% quickStats('NUT_CHLA') %>%
          mutate(NUT_CHLA_STAT = NA) } # don't show a real assessment decision
    
    # run DO daily average status for riverine and tuck results into comments later
   # if(unique(stationData$lakeStation) == TRUE){
  #    DO_Daily_Avg_STAT <- ''
  #  } else {
    
    # Run DO Daily Avg for everyone!
      DO_Daily_Avg_STAT <- paste0('DO_Daily_Avg_STAT: ', 
                                       DO_Assessment_DailyAvg(stationData) %>% 
                                         quickStats('DO_Daily_Avg') %>% 
                                         dplyr::select(DO_Daily_Avg_STAT) %>% pull())#}
    
    results <- cbind(
      StationTableStartingData(stationData),
      tempExceedances(stationData) %>% quickStats('TEMP'),
      DOExceedances_Min(stationData) %>% quickStats('DO'), 
      # this will be removed for lake stations later since it does not apply
      pHExceedances(stationData) %>% quickStats('PH'),
      bacteriaAssessmentDecisionClass(stationData),
      
      #bacteriaAssessmentDecision(stationData, 'ECOLI', 'RMK_ECOLI', 10, 410, 126) %>%
      #  dplyr::select(ECOLI_EXC:ECOLI_STATECOLI_VERBOSE), # add verbose comment to COMMENT field
      #bacteriaAssessmentDecision(stationData, 'ENTEROCOCCI', 'RMK_ENTEROCOCCI', 10, 130, 35) %>%
      #  dplyr::select(ENTER_EXC:ENTER_STATENTER_VERBOSE), # add verbose comment to COMMENT field

      # Ammonia needs to go here
      ammoniaDecision(list(acute = freshwaterNH3Assessment(ammoniaAnalysisStation, 'acute'),
                           chronic = freshwaterNH3Assessment(ammoniaAnalysisStation, 'chronic'),
                           fourDay = freshwaterNH3Assessment(ammoniaAnalysisStation, 'four-day'))), 
    
      # Flag for whether or not metals data exists
      metalsData(filter(WCmetals, Station_Id %in% stationData$FDT_STA_ID), 'WAT_MET'),
      # old method when Roger made the assessment for group
      #metalsExceedances(filter(WCmetals, FDT_STA_ID %in% stationData$FDT_STA_ID) %>% 
      #                                 dplyr::select(`ANTIMONY HUMAN HEALTH PWS`:`ZINC ALL OTHER SURFACE WATERS`), 'WAT_MET'),
      # Mark's water column PCB results, flagged
      WCtoxics, # from above, adds in PWS and water column PCB information
      # PCBmetalsDataExists(filter(markPCB, str_detect(SampleMedia, 'Water')) %>%
      #                                 filter(StationID %in% stationData$FDT_STA_ID), 'WAT_TOX'), 
      # 
      # Roger's sediment metals analysis, transcribed
      metalsData(filter(Smetals, Station_Id %in% stationData$FDT_STA_ID), 'SED_MET'),
      # old method when Roger made the assessment for group
      # metalsExceedances(filter(Smetals, FDT_STA_ID %in% stationData$FDT_STA_ID) %>% 
      #                                 dplyr::select(ARSENIC:ZINC), 'SED_MET'), 
      # 
      # Mark's sediment PCB results, flagged
      PCBmetalsDataExists(filter(markPCB, str_detect(SampleMedia, 'Sediment')) %>%
                      filter(StationID %in% stationData$FDT_STA_ID), 'SED_TOX'),
      
      # Gabe's fish metals results, flagged
      PCBmetalsDataExists(filter(fishMetals, Station_ID %in% stationData$FDT_STA_ID), 'FISH_MET'),
      
      # Gabe's fish PCB results, flagged
      PCBmetalsDataExists(filter(fishPCB, `DEQ rivermile` %in%  stationData$FDT_STA_ID), 'FISH_TOX'),

    
      # Benthics 
      # BENTHIC_STAT, BENTHIC_WOE_CAT, BIBI_SCORE
      benthicAssessment(stationData, VSCIresults),
      
      # Nutrient Assessment done above by waterbody type
      TP,
      chla) %>%
    
      # # Nutrient: TP (lakes have real standards; riverine no longer uses 0.2 mg/L as an observed effect for Aquatic life use)
      # # NUT_TP_EXC, NUT_TP_SAMP, NUT_TP_STAT
      # countNutrients(stationData, PHOSPHORUS_mg_L, LEVEL_PHOSPHORUS, NA) %>% quickStats('NUT_TP') %>% 
      #   mutate(NUT_TP_STAT = ifelse(NUT_TP_STAT != "S", "Review", NA)), # flag OE but don't show a real assessment decision
      # 
      # # Nutrients: Chl a (lakes)
      # #NUT_CHLA_EXC, NUT_CHLA_SAMP, NUT_CHLA_STAT
      # countNutrients(stationData, CHLOROPHYLL_A_ug_L, LEVEL_CHLOROPHYLL_A, NA) %>% quickStats('NUT_CHLA') %>%
      #   mutate(NUT_CHLA_STAT = NA)) %>% # don't show a real assessment decision
    
      # COMMENTS
      mutate(COMMENTS = paste0(DO_Daily_Avg_STAT, 
                               ' | AMMONIA Comment: ', `Assessment Decision`)) %>% #,
             #BACTERIA_COMMENTS = paste0(' | E.coli Comment: ', ECOLI_STATECOLI_VERBOSE,
            #                   ' | Enterococci Comment: ', ENTER_STATENTER_VERBOSE)) %>%
      left_join(comments, by = 'STATION_ID') %>%
      dplyr::select(-ends_with(c('exceedanceRate', 'VERBOSE', 'Assessment Decision', 'StationID'))) %>%  # to match Bulk Upload template but helpful to keep visible til now for testing
      mutate(`Date Last Sampled` = dateLastSampled)
  } else {# pull what you can from last cycle and flag as carry over
    results <- filter(stationTable, STATION_ID == stationTable$STATION_ID[i]) %>%
      dplyr::select(STATION_ID:VAHU6, COMMENTS) %>%
      mutate(COMMENTS = 'This station has no data in current window but was carried over due to IM in one of the 2020IR status fields or the 2020 stations table reports the station was carried over from a previous cycle.') %>%#,
             #BACTERIA_COMMENTS = NA) %>%
      left_join(comments, by = 'STATION_ID') %>% 
      mutate(`Date Last Sampled` = dateLastSampled)
  }
  
  stationTableResults <- bind_rows(stationTableResults,results)
  ammoniaAnalysis <- bind_rows(ammoniaAnalysis, tibble(StationID = unique(stationData$FDT_STA_ID), AmmoniaAnalysis = list(ammoniaAnalysisStation)))
    
}


stationTableResults <- bind_rows(stationsTemplate, stationTableResults) %>%
  # for now bc bacteria needs help still
  dplyr::select(STATION_ID:`2018 IR COMMENTS`)

timeDiff = Sys.time()- startTime
  
```