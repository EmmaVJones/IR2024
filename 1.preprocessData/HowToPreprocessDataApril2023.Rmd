---
title: "Statewide Data Preprocessing"
author: "Emma Jones"
date: "7/22/2022"
output: html_document
---

# Intro

After an update to lwgeom, this entire script needs to be run in R4.0.3 or you run into 'C Stack limit' errors at geospatial processes. Likely this has to do with updates to geospatial operations in underlying system dependencies with sf0.8 to sf 0.9, but not entirely sure.

All other scripts should be performed in R 3.6.2 to ensure they work on the server (which is still running GDAL 2.2X and PROJ4)


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(sf)
library(readxl)
library(lubridate)
#library(lwgeom) # used in assessmentLayerSubbasinJoin missingSites step
```

```{r connect to server}
library(pins)
library(config)
library(purrr)
library(lubridate)

# Connect to server
conn <- config::get("connectionSettings") # get configuration settings

board_register_rsconnect(key = conn$CONNECT_API_KEY,  #Sys.getenv("CONNECT_API_KEY"),
                         server = conn$CONNECT_SERVER)#Sys.getenv("CONNECT_SERVER"))

```

This project is a outlines how to preprocess data for the manual WQS and AU attribution process. 

Each step will be written as individual modules where lengthy code to complete each step is stored elsewhere and sourced for use. This method of organizing the project helps with project maintenance and troubleshooting specific problem areas when they occur.

# Water Quality Standards Data Preprocessing

## WQS Split up

After each update of WQS layers, it is important to split these layers into bite size consumption for the review app such that we don't send unnecessary data to/from servers and speed up rending time. All snapping will be completed against the official WQS layers, but for app purposes, we will only bring in WQS in a selected subbasin.

Because most (all) DEQ subbasin layers incorrectly attribute the upper Potomac basin inside the Shenandoah in VRO, we will create our own layer to work from that fixes this error. 

```{r fix subbasin layer}
#source('./preprocessingModules/newDEQsubbasinLayerCreation.R') # only run once as needed
```

That really only needs to be done once such that 'GIS/DEQsubbasins_EVJ.shp' is created. This is the layer the splitWQSbySubbasin.R will use.

Now we take that newly created layer and use it to split up each of the riverine, lakes, and both types of estuary layers appropriately.


```{r splitWQS by subbasin for app}
#source('./preprocessingModules/splitWQSbySubbasin.R') # only run once as needed
```

And one last step to link basins to VAHU6 information for easier filtering inside the apps.

```{r build lookuptable for VAHU6 to subbasin}
#source('./preprocessingModules/VAHU6toSubbasin.R')
```


Now we can move on to actual data processing.




# Data Organization

Now we will create a list of all stations that are relevant to the IR 2024 assessment window. They are not all pulled from the same sources, so multiple datasets will be combined into a single object named distinctSites that we will then compare to existing WQS and AU information. Where stations in our distinctSites object lack either of these pieces of metadata, we will attribute them in this script.


### Conventionals Data

The largest dataset needed for these analyses is the conventionals dataset that Roger Stewart pulls every two years for each IR window. This is inclusive of data pulled from CEDS that should be used in the assessment. 


```{r conventionals final, echo=F, warning=F, error=FALSE}
conventionalsRaw <- read_excel('data/finalData/CEDSWQM_2024_IR_20230302.xlsx', sheet = 'CONVENTIONALS')

# before removing data from your environment, here is how to clean Roger's data up to match with schema expected by automated tools
#source('preprocessingModules/updateConventionalsSchema.R') # version working with old names, works for Feb 16 data pull
# pin final versions
# pin(conventionals, name = 'ejones/conventionals2024final',
#     description = 'Final IR2024 conventionals dataset, EVJ reorganized from Roger 3/03/2023.',
#     board = 'rsconnect')

conventionals <- pin_get('ejones/conventionals2024final', board = 'rsconnect') 


# An object named conventionals is now in your environment following the cleanup step above

```


It is important to troubleshoot and develop as much as possible prior to the final data pull. Below is how draft data pulls were used when the final data was not yet available.

```{r conventionals draft, echo=F, warning=F, error=FALSE, eval=FALSE}
# **Draft**
# 
# This is the February draft data pull that will help expedite the process when the official Conventionals data pull is released in March 2023. 
#conventionalsRaw <- read_excel('data/draftData/CEDSWQM_2024_IR_20230216.xlsx', sheet = 'CONVENTIONALS')
  #read_excel('data/draftData/CONVENTIONALS_20230216.xlsx', sheet = 'CONVENTIONALS') 
  #read_excel('data/draftData/CEDSWQM_2024_IR_20230209.XLSX', sheet = 'CONVENTIONALS') 

# before removing data from your environment, here is how to clean Roger's data up to match with schema expected by automated tools
#source('preprocessingModules/updateConventionalsSchema.R') # version working with old names, works for Feb 16 data pull
#source('preprocessingModules/updateConventionalsSchemaAndPinToServer.R')  # version working with new storet names, works for Feb 9 data pull
# pin final versions
# pin(conventionals, name = 'ejones/conventionals2024draft',
#     description = 'Working IR2024 conventionals dataset, EVJ reorganized from Roger 2/23/2023.',
#     board = 'rsconnect')

#conventionals <- pin_get('ejones/conventionals2024draft', board = 'rsconnect') 


# An object named conventionals is now in your environment to following the cleanup step above

```

#### distinctSites

Create a new object called distinctSites. This will be useful for keeping track of all anticipated sites for the new cycle. We will use this object to check which sites need WQS and which need AU. 

```{r conventionals missing WQS}
conventionals <- conventionals %>% 
  dplyr::select(FDT_STA_ID, GROUP_STA_ID, STA_DESC, Data_Source, Latitude, Longitude) %>% 
  distinct(FDT_STA_ID, .keep_all = T)

distinctSites <- conventionals
#dplyr::select(conventionals, -c(Latitude, Longitude)) # object to save unique sites for later, use standardized other lat/lng
```


Go to server and grab any station details that might be helpful.

```{r station spatial metadata}
WQMstationsView <- pin_get('ejones/WQM-Stations-View', board = 'rsconnect') %>% 
  dplyr::select(Sta_Id, Sta_Lv1_Code: Sta_Huc_Code)
WQMstations <- pin_get('ejones/WQM-Stations-Spatial', board = 'rsconnect') %>% 
  left_join(WQMstationsView, by = c('StationID' = 'Sta_Id'))
rm(WQMstationsView)

distinctSites1 <- left_join(distinctSites, WQMstations, by = c('FDT_STA_ID' = 'StationID')) %>% 
  distinct(FDT_STA_ID, .keep_all = T) %>% 
  mutate(STA_DESC = coalesce(STA_DESC, Sta_Desc),
         Latitude = coalesce(Latitude.x, Latitude.y),
         Longitude = coalesce(Longitude.x, Longitude.y)) %>% 
  dplyr::select(-c(Sta_Desc, Latitude.x, Latitude.y, Longitude.x, Longitude.y)) %>%  # remove duplicate column
  dplyr::select(FDT_STA_ID, GROUP_STA_ID, STA_DESC, Data_Source, Latitude, Longitude, everything())
  
# make sure we didn't lose anyone
distinctSites$FDT_STA_ID[! distinctSites$FDT_STA_ID %in% distinctSites1$FDT_STA_ID]

#rename objects to keep things organized
distinctSites <- distinctSites1
# clean up workspace 
rm(distinctSites1);rm(WQMstations)


# make sure all stations have a location, save any that don't for later citmon cleanup steps
missingCoordinates <- filter(distinctSites, is.na(Latitude) | is.na(Longitude)) 
  

# make a spatial dataset for later use
distinctSites_sf <- distinctSites %>% 
  filter(! FDT_STA_ID %in% missingCoordinates$FDT_STA_ID) %>% 
  st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
            remove = F, # don't remove these lat/lon cols from df
            crs = 4326)
# save for later just in case
#saveRDS(distinctSites_sf, './data/distinctSites_sf03072023.RDS')
```



### Other data: Citizen Monitoring and Non Agency


Reid and Joe organized citizen monitoring data for 2017-2020. Here is how we smash together the results.

```{r citmon smash}
#source('preprocessingModules/citmonDataSmash.R')
```


This Rmd and my R session were having serious troubles at this point. Bringing running any of the spatial snapping processes proved impossible due to constant "C stack limit" errors. The 2021-2022 citmon/non agency data from Reid was thus organized in a separate project in 1.preprocessData/CitmonOrganization/



#### 2021-2022 CMC Site information

In March 2023, Reid pulled all stations in the CMC sampled during 2021-2022. He joined all available existing DEQ StationIDs to these sites, but there were still a fair number that needed this information because they were new to this cycle. Emma took that information and ran a number of spatial joins to suggest AU and WQS information for these sites (where this information wasn't available from previous assessments). This work was given to the regional assessment staff to review the joined FDT_STA_ID (or rename the placeholder ID with a desired FDT_STA_ID), WQS_ID, and ID305B. The script sourced below picks up where we pull this reviewed information and pin the metadata to the server.

```{r citmon 2021 2022 metadata additions}
#source('preprocessingModules/citmon20212022stationMetadataOrganization.R')
```




## Citizen Monitoring WQS for older stations attributed at AU level

For a lot of older citmon stations, we should have WQS info at AU level that we can backdoor to use for IR2024. going forward with new sites, we will only use the official metadata app for WQS and AU snapping, but here is a good way to glean data that already exists.

```{r}
#source('preprocessingModules/specialStepForIR2024toImportWQSinfoAttributedAtAUlevelFromODS.R')
```





Organize data to match distinctSites_sf format

```{r citmonNonagency spatial}
citmonNonAgency_sf <- filter(distinctSites_sf, is.na(ASSESS_REG)| is.na(US_L3NAME)) %>%  #citmonNonAgency %>% 
  dplyr::select(FDT_STA_ID, GROUP_STA_ID, STA_DESC, Data_Source,
                Latitude, Longitude) %>% 
  distinct(FDT_STA_ID, .keep_all = T) %>% 
   st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
           remove = F, # don't remove these lat/lon cols from df
           crs = 4326)
```


Need to add spatial metadata to make next steps work more efficiently.

```{r citmonNonagency spatial metadata}
#Bring in extra large ecoregion and assessment region files. By extra large we mean that they are continuous outside the state boundary such that stations that fall outside the state are not removed when spatially intersected
ecoregion4Large <- st_read('C:/HardDriveBackup/R/GitHub/pinData/data/GIS/VA_level4ecoregion_WGS84.shp')
vahu6 <- st_read('C:/HardDriveBackup/R/GitHub/pinData/data/GIS/VA_SUBWATERSHED_6TH_ORDER_STG.shp') # this version of vahu6 layer goes outside state boundary
subbasinConversion <- read_csv('C:/HardDriveBackup/R/GitHub/pinData/data/subbasinToVAHU6conversion.csv')

subbasinLayer <- st_read('../GIS/DEQ_VAHUSB_subbasins_EVJ.shp')  %>% 
  st_drop_geometry() %>% 
  rename('SUBBASIN' = 'SUBBASIN_1') %>% 
  distinct(subbasin, .keep_all = T) %>%  # don't need this by Assessment Region right now so simplify dataset to make next join cleaner
  dplyr::select(BASIN_NAME, BASIN_CODE, SUBBASIN, subbasin)

unrestrictedAssessmentRegionVAHU6Subbasin <- left_join(vahu6, subbasinConversion, by = c('VAHU6', 'VAHU5')) %>% 
  left_join(subbasinLayer, by = c('BASIN_CODE' = 'BASIN_CODE',
                                  'VAHUSB' = 'subbasin'))
#View(unrestrictedAssessmentRegionVAHU6Subbasin %>% st_drop_geometry())

# need this if running just citmon
#distinctSites_sf <- readRDS('./data/distinctSites_sf02232023.RDS')


# Add Level 3 and 4 Ecoregion Info to citmon and any conventionals stations taht don't have info
citmonNonAgency_sf1 <- rbind(citmonNonAgency_sf,
                             filter(conventionals, FDT_STA_ID %in% missingCoordinates$FDT_STA_ID) %>% 
                               st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
                                        remove = F, # don't remove these lat/lon cols from df
                                        crs = 4326)) %>% 
                             
  st_intersection(unrestrictedAssessmentRegionVAHU6Subbasin) %>% 
  st_intersection(dplyr::select(ecoregion4Large, US_L3CODE, US_L3NAME, US_L4CODE, US_L4NAME)) 

# add back in sites that could have been lost in spatial joins
citmonNonAgency_sf2 <- bind_rows(citmonNonAgency_sf1 %>% st_drop_geometry(), # drop geom so bind_rows will work instead of rbind
                                 filter(citmonNonAgency_sf, ! FDT_STA_ID %in% citmonNonAgency_sf1$FDT_STA_ID) %>% 
                                   st_drop_geometry()) %>% 
  # add back in DEQ stations that might not have made the spatial join cut
  bind_rows(filter(conventionals, FDT_STA_ID %in% missingCoordinates$FDT_STA_ID) %>% 
              filter( ! FDT_STA_ID %in% citmonNonAgency_sf1$FDT_STA_ID)) %>% 
  
  # change back to spatial object
   st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
           remove = F, # don't remove these lat/lon cols from df
           crs = 4326)

# make sure no duplicates
citmonNonAgency_sf2 %>% group_by(FDT_STA_ID) %>% mutate(n = n()) %>% filter(n > 1) %>% View()


# QA step
# filter(citmonNonAgency_sf, ! FDT_STA_ID %in% citmonNonAgency_sf2$FDT_STA_ID)
# mapview::mapview(filter(citmonNonAgency_sf, ! FDT_STA_ID %in% citmonNonAgency_sf1$FDT_STA_ID))+
#   mapview::mapview(ecoregion4Large)

distinctSites_sf <- bind_rows(distinctSites_sf %>% st_drop_geometry() %>% 
                                 filter(!FDT_STA_ID %in% citmonNonAgency_sf2$FDT_STA_ID),
                              citmonNonAgency_sf2 %>% st_drop_geometry() %>% 
                                dplyr::select(any_of(names(distinctSites_sf)))) %>%
  dplyr::select(names(distinctSites_sf %>% st_drop_geometry())) %>% # once you have the right column names, reorder to correct order
  st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
           remove = F, # don't remove these lat/lon cols from df
           crs = 4326) %>% 
  distinct(FDT_STA_ID, .keep_all = T) %>% 
 # mutate(GROUP_STA_ID = NA_character_) %>% #will need this when it comes time to do real citmon stuff
  dplyr::select(FDT_STA_ID, GROUP_STA_ID, everything())

# make sure no duplicates
distinctSites_sf %>% group_by(FDT_STA_ID) %>% mutate(n = n()) %>% dplyr::select(n, everything()) %>%  filter(n > 1) %>%   View()
# duplicates are all legit. These are colocated stations with citizen monitoring. KEEP THESE

# Clean up workspace
rm(list = c('ecoregion4Large','county', 'vahu6','subbasinConversion','unrestrictedAssessmentRegionVAHU6Subbasin',
            'citmonNonAgency_sf1', 'citmonNonAgency_sf2', 'path', 'subbasinLayer'))
```


### Remove Unwanted Stations

There are some stations that are deemed permanently inappropriate for assessment purposes that make it through the conventionals query. Remove these stations now so assessors don't need to see them in the attribution or further steps.

```{r staiton hit list}
stationHitList <- read_csv('data/stationHitList.csv')

distinctSites_sf <- filter(distinctSites_sf, ! FDT_STA_ID %in% stationHitList$`StationID to Remove`)


#saveRDS(distinctSites_sf, 'data/distinctSites_sf03072023.RDS')#data/distinctSites_sf_withCitmon02232023.RDS')
```





### Attach subbasin information

Assessment region is already included in the attached metadata thanks to the previous join to WQMstations. This information is important for processing each region through a loop for AU connection and WQS attachment. Subbasin information is important for WQS processing.

lwgeom needs sf >= 0.9.3, so this operation needs to happen in R 4.0.3. Unless you have changed any inputs prior to this point, just keep with 3.6.2 and read in data already processed.

```{r assessment and subbasin join}
#source('preprocessingModules/assessmentLayerSubbasinJoin.R') # lwgeom needs sf >= 0.9.3, so this operation needs to happen in R 4.0.3
distinctSites_sf <- readRDS('./data/distinctSites_sf_withCitmon03072023.RDS') #distinctSites_sf_withCitmon02232023.RDS')
distinctSitesToDo <- distinctSites_sf 
# keep a copy of original distinct sites to check that no one was lost at the end
``` 

**Note the VAHU6 data is derived spatially, so this is a good first step, but when there is human QAed VAHU6 data available, we will use that data instead. Sometimes assessors use stations outside a VAHU6 to assess said VAHU6.**


Save missingSites data for app. This is critical for those sites that fall out of assessment layer boundary that still need work. This will be brought in by app for assessor to use.

For 2024IR draft data, we didn't have any missingSites.

```{r missingSites for app}
#saveRDS(missingSites, 'data/missingSites.RDS')
```



### Pin Conventionals and Conventionals Distinct to server

2/13/2023- so this is confusing, where did conventionals_distinct come from? distinctSites_sf data structure doesn't match what is on server. also conventionals data was removed from environment. skipping for now as to not mess more things up

```{r}
# conventionals_distinct <- conventionals_distinct %>% 
#   filter(!is.na(Latitude) | !is.na(Longitude)) %>% 
#   st_as_sf(coords = c("Longitude", "Latitude"), 
#                remove = F, # don't remove these lat/lon cols from df
#                crs = 4326) %>% 
#   st_intersection(dplyr::select(basins, BASIN_CODE)) 
# pin('ejones/conventionals2024_distinctdraft', board = 'rsconnect')  # see below for fix coming this way at final stage

```

now pin to server


```{r}
# pin(distinctSites_sf, 'ejones/conventionals2024_distinctdraft', board = 'rsconnect',
#     description = 'Working IR2024 conventionals_distinct dataset based on conventionals final pull from Roger and 2017-2020 citmon/nonagency data from Reid that have FDT_STA_ID')  # see below for fix coming this way at final stage

```



# Attach WQS to Stations

Now we will work with our distinctSites_sf object to spatially join WQS information where it doesn't already exist. First, we need to establish which stations already have WQS_ID's so we don't duplicate efforts. This information is stored on the R server. 

```{r WQS info on server}
WQStableExisting <- pin_get('ejones/WQSlookup', board = 'rsconnect')

citmonWQS <- pin_get("ejones/citmonStationsWithWQSFinal", board = "rsconnect")
```


#### Aside, how to get WQS data from the server

To pull WQS information QAed in the metadata app and saved on the server, first go in to Connect and identify the app number of the current metadata application (under Regional Assessment Metadata Validation Tool- source versions). This information is critical to locating the area where info saved on the server. 

Next, in putty, see what info is available in that directory sudo  ls -l /app/rstudio-connect/apps/[appNumber]/[sourceVersion]/WQSlookupTable where [appNumber] = the unique application number from Connect
where [sourceVersion] = the current build number from Connect.
e.g. sudo  ls -l /app/rstudio-connect/apps/336/9040/WQSlookupTable

Next, in filezilla, make sure the last version of this data pulled down is safely renamed by date to make sure grabbing new data doesnt overwrite old data. To do that, /home/aaejones/temp/ rename WQSlookupTable to WQSlookupTableFILEDATE (e.g. WQSlookupTable12152020).

Then in putty, copy whole directory and change all file permissions for download
sudo cp -arv /app/rstudio-connect/apps/[appNumber]/[sourceVersion]/WQSlookupTable /home/aaejones/temp
sudo chmod -R 755 /home/aaejones/temp/WQSlookupTable

e.g. sudo cp -arv /app/rstudio-connect/apps/336/9040/WQSlookupTable /home/aaejones/temp
e.g. sudo chmod -R 755 /home/aaejones/temp/WQSlookupTable

Then in filezilla, download the new WQSlookupTable directory to local system.

These are pulled directly from WQSLookupTable directories and can contain issues
```{r}
#source('./preprocessingModules/pullWQSID.R') # this pulls all stations in WQSLookupTable directory, which could include duplicates
```

#### End WQS data from server aside

## Remove stations that have WQS information from "to do" list

Identify real number of sites that need spatial joining.

```{r find where spatial joins have already happened}
distinctSitesToDoWQS <- filter(distinctSitesToDo, ! FDT_STA_ID  %in% WQStableExisting$StationID) %>% 
  filter(! FDT_STA_ID  %in% citmonWQS$StationID)
#write.csv(distinctSitesToDoWQS, 'needsWQS.csv')
rm(WQStableExisting);rm(citmonWQS) # save all memory you can
```


#### Spatially Join WQS

Since transitioning the WQS storage from local (individual assessor files) to a centralized system (on the R server for multiple program uses), the number of stations that require WQS attribution should decrease significantly each IR cycle. To attribute each station to WQS information, we first need to do all spatial joins to new WQS layer to get appropriate UID information, then we can send that information to an interactive application for humans to manually verify.

Here is the table used to store link information from stations to appropriate WQS.

```{r WQStable}
# saveRDS(distinctSitesToDoWQS, 'data/distinctSitesToDoWQS.RDS')
# distinctSitesToDoWQS <- readRDS('data/distinctSitesToDoWQS.RDS')
WQStable <- tibble(StationID = NA, WQS_ID = NA)
```


Since it is much faster to look for spatial joins by polygons compared to snapping to lines, we will run spatial joins by estuarine polys and reservoir layers first.

##### Estuarine Polygons

Find any sites that fall into an estuary WQS polygon. This method is only applied to subbasins that intersect estuarine areas. This process also removes any estuarine sites from the data frame of unique sites that need WQS information (distinctSitesToDoWQS).

We will bring in (source) a custom function that runs the analysis for us, feed it our latest WQS information, and let the function run across all input stations.

```{r estuary methods}
source('preprocessingModules/WQS_estuaryPoly.R')

# Bring in estuary layer
estuarinePolys <- st_read('../GIS/draft_WQS_shapefiles_2022/estuarinepolygons_draft2022.shp', 
                          fid_column_name = "OBJECTID") %>%
  st_transform(4326)
  
WQStable <- estuaryPolygonJoin(estuarinePolys, distinctSitesToDoWQS, WQStable)

rm(estuarinePolys) # clean up workspace
```

Remove stations that fell inside estuarine polygons from the 'to do' list.

```{r remove estuary poly sites}
distinctSitesToDoWQS <- filter(distinctSitesToDoWQS, ! FDT_STA_ID %in% WQStable$StationID)
```


##### Lake Polygons

Find any sites that fall into a lake WQS polygon. This method is applied to all subbasins at once as it is a simple spatial join. 
The next step removes any lake sites from the data frame of unique sites that need WQS information (distinctSitesToDoWQS).


```{r lake methods}
source('preprocessingModules/WQS_lakePoly.R')

lakesPoly <- st_read('../GIS/draft_WQS_shapefiles_2022/lakes_reservoirs_draft2022.shp',
                     fid_column_name = "OBJECTID") %>%
  st_transform(4326)

WQStable <- lakePolygonJoin(lakesPoly, distinctSitesToDoWQS, WQStable)

rm(lakesPoly) # clean up workspace
```

Remove stations that fell inside lake polygons from the 'to do' list.

```{r remove estuary poly sites}
distinctSitesToDoWQS <- filter(distinctSitesToDoWQS, ! FDT_STA_ID %in% WQStable$StationID)
```



### Spatially Join WQS Lines

Now on to the more computationally heavy WQS line snapping methods. First we will try to attach riverine WQS, and where stations remain we will try the estuarine WQS snap.

##### Riverine Lines

To do this join, we will buffer all sites that don't fall into a polygon layer by a set sequence of distances. The output will add a field called `Buffer Distance` to the WQStable to indicate distance required for snapping. This does not get transported to the data of record, but it is useful to keep for now for QA purposes. If more than one segment is found within a set buffer distance, then many rows will be attached to the WQStable with the single identifying station name. It is up to the QA tool to help the user determine which of these UID's are correct and drop the other records.

We then remove any riverine sites from the data frame of unique sites that need WQS information (distinctSitesToDoWQS).

```{r riverine methods}
source('snappingFunctions/snapWQS.R')

riverine <- st_read('../GIS/draft_WQS_shapefiles_2022/riverine_draft2022.shp',
                    fid_column_name = "OBJECTID") #%>% 
  #st_transform(102003) # forcing to albers from start bc such a huge layer
  #st_transform('+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=37.5 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m no_defs')
# 2/3/21 have to manually feed character proj4string bc doesn't recognize albers epsg 

WQStable <- snapAndOrganizeWQS(distinctSitesToDoWQS, 'FDT_STA_ID', riverine, 
                               bufferDistances = seq(20,80,by=20),  # buffering by 20m from 20 - 80 meters
                               WQStable)
#saveRDS(WQStable, 'data/WQStableCitmon.RDS')

rm(riverine)
```

Remove stations that attached to riverine segments from the 'to do' list.

```{r remove estuary poly sites}
distinctSitesToDoWQS <- filter(distinctSitesToDoWQS, ! FDT_STA_ID %in% WQStable$StationID)
```


We can use this one last opportunity to test stations that didn't connect to the riverine WQS against the estuarine lines WQS as a one last hope of attributing some WQS information. We will take all stations from the WQStable that didn't snap to any WQS segments (`Buffer Distance` =='No connections within 80 m') and add those back in to our distinctSitesToDoWQS list to try to snap them to the estuarine lines spatial data. 

```{r no riverine snaps}
distinctSitesToDoWQS <- filter(WQStable, `Buffer Distance` =='No connections within 80 m') %>%
  left_join(dplyr::select(distinctSites_sf, FDT_STA_ID, Latitude, Longitude, SUBBASIN) %>%
              rename('StationID'= 'FDT_STA_ID'), by='StationID') %>%
  dplyr::select(-c(geometry)) %>%
  st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
           remove = T, # don't remove these lat/lon cols from df
           crs = 4326)
                            
```



##### Estuarine Lines

If a site doesn't attach to a riverine segment, our last step is to try to attach estuary line segments before throwing an empty site to the users for the wild west of manual QA. Only send sites that could be in estuarine subbasin to this function to not waste time.
Removes any estuary lines sites from the data frame of unique sites that need WQS information.

```{r estuarine lines methods}
source('snappingFunctions/snapWQS.R')

estuarineLines <- st_read('../GIS/draft_WQS_shapefiles_2022/estuarinelines_draft2022.shp' , fid_column_name = "OBJECTID") #%>%
  #st_transform(102003) # forcing to albers from start bc such a huge layer
  #st_transform(4326)

# Only send sites to function that could be in estuarine environment
WQStable <- snapAndOrganizeWQS(filter(distinctSitesToDoWQS, SUBBASIN %in% c("Potomac River",
                                                                            "Rappahannock River", 
                                                                            "Atlantic Ocean Coastal",
                                                                            "Chesapeake Bay Tributaries",
                                                                            "Chesapeake Bay - Mainstem",
                                                                            "James River - Lower",  
                                                                            "Appomattox River",
                                                                            "Chowan River", 
                                                                            "Atlantic Ocean - South" ,
                                                                            "Dismal Swamp/Albemarle Sound")),
                               'StationID', estuarineLines, 
                               bufferDistances = seq(20,80,by=20),  # buffering by 20m from 20 - 80 meters
                               WQStable)
#saveRDS(WQStable, 'WQStable2.RDS')
rm(estuarineLines)
```

Remove stations that attached to estuarine segments from the 'to do' list.

```{r remove estuary poly sites}
distinctSitesToDoWQS <- filter(distinctSitesToDoWQS, ! StationID %in% WQStable$StationID)
```


Double check no stations were lost in these processes. 

```{r double check no one lost}
#Make sure all stations from original distinct station list have some sort of record (blank or populated) int the WQStable.

distinctSitesToDoWQS <- filter(distinctSitesToDo, ! FDT_STA_ID  %in% WQStableExisting$StationID)
distinctSitesToDoWQS$FDT_STA_ID[!(distinctSitesToDoWQS$FDT_STA_ID %in% unique(WQStable$StationID))]
```

And that none of the missing sites (sites that fall outside of the assessmentLayer) are not missing from the WQS snapping process.

```{r missingSites got WQS}
missingSites <- readRDS('data/missingSites.RDS')
missingSites$FDT_STA_ID[!(missingSites$FDT_STA_ID %in% unique(WQStable$StationID))]
```


### Assign something to WQS_ID so sites will not fall through the cracks when application filtering occurs

We don't want to give everyone all the stations that didn't snap to something, so we need to at least partially assign a WQS_ID so the stations get into the correct subbasin on initial filter.

If a station snapped to nothing, we will assigning it a RL WQS_ID and subbasin it falls into by default.

```{r blank WQS_ID partially filled in}
# for DEQ stations!
WQStableMissing <- filter(WQStable, is.na(WQS_ID)) %>%
  # drop from list if actually fixed by snap to another segment
  filter(! StationID %in% filter(WQStable, str_extract(WQS_ID, "^.{2}") %in% c('EL','LP','EP'))$StationID) %>%
  left_join(dplyr::select(distinctSites_sf, FDT_STA_ID, BASIN_CODE) %>%
              st_drop_geometry(), by = c('StationID' = 'FDT_STA_ID')) %>%
  # some fixes for missing basin codes so they will match proper naming conventions for filtering
  mutate(BASIN_CODE1 = case_when(is.na(BASIN_CODE) ~ str_pad(
    ifelse(grepl('-', str_extract(StationID, "^.{2}")), str_extract(StationID, "^.{1}"), str_extract(StationID, "^.{2}")), 
    width = 2, side = 'left', pad = '0'),
    TRUE ~ as.character(BASIN_CODE)),
    BASIN_CODE2 = str_pad(BASIN_CODE1, width = 2, side = 'left', pad = '0'),
    WQS_ID = paste0('RL_', BASIN_CODE2,'_NA')) %>%
  dplyr::select(-c(BASIN_CODE, BASIN_CODE1, BASIN_CODE2))

# for citmon/nonAgency stations that don't have correct basin info
subbasinLayer <- st_read('../GIS/DEQ_VAHUSB_subbasins_EVJ.shp') 
WQStableMissing1 <- WQStableMissing %>% 
  left_join(dplyr::select(distinctSites_sf, FDT_STA_ID, Latitude, Longitude) %>% st_drop_geometry(),
            by = c('StationID' = 'FDT_STA_ID')) %>% 
  st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
           remove = F, # don't remove these lat/lon cols from df
           crs = 4326) %>% 
  st_intersection(subbasinLayer) %>% 
  dplyr::select(StationID, WQS_ID, `Buffer Distance` = "Buffer.Distance", BASIN_CODE) %>% 
  mutate(WQS2 = paste0(str_extract(WQS_ID, "^.{2}"),
                       '_', 
                       str_pad(BASIN_CODE, width = 2, side = 'left', pad = '0'),
                      '_NA')) %>% 
  dplyr::select(StationID, WQS_ID = WQS2, `Buffer Distance`) %>% 
  st_drop_geometry()

WQStable <- filter(WQStable, !is.na(WQS_ID)) %>% 
  filter(! StationID %in% WQStableMissing1$StationID) %>%
  bind_rows(WQStableMissing)

saveRDS(WQStable, 'data/WQStableCitmonAll.RDS')
```





### QA Spatial Attribution Process

See how many sites snapped to too many WQS segments

```{r snapCheck}
tooMany <- snapCheck(WQStable)

# quick snap check previous WQS attributed data from assessors bc some issues have been noticed
tooMany_WQStableExisting <- snapCheck(WQStableExisting) %>%
  dplyr::select(-Comments) # delete this so we can sent it back to user to be fixed

WQStable <- bind_rows(WQStable, tooMany_WQStableExisting)


fine <- filter(WQStable, ! (StationID %in% tooMany$StationID)) %>% 
  filter(`Buffer Distance` %in% c(NA,  "20 m", "40 m", "60 m" ,"80 m"))
none <- filter(WQStable, ! StationID %in% tooMany$StationID) %>%
  filter(`Buffer Distance` == "No connections within 80 m")

# quick stats 
(nrow(fine) / nrow(distinctSites)) * 100 # ~ 74% snapped to one segment
(length(unique(tooMany$StationID)) / nrow(distinctSites)) * 100 # ~ 23% need extra manual review
(nrow(none) / nrow(distinctSites)) * 100 # ~ 4% snapped to one segment

rm(fine);rm(none);rm(tooMany)
```

skipped some stuff. double check when have citmon etc if needed


Save WQStable for manual review in application. 
```{r save WQS for app review}
saveRDS(WQStable, './data/WQStable11032022.RDS')

# clean up workspace
rm(list = c('distinctSitesToDoWQS','WQStable', 'WQStableExisting','WQStableMissing'))
```

Add citmon to all DEQ stations needing WQS.

```{r}
WQSog <- readRDS('./data/WQStable10182022.RDS')# last time pre citmon

WQStogether <- bind_rows(WQSog, WQStable)
#saveRDS(WQStogether, 'data/WQStableWithCitmon11142022.RDS')
```


Pin spatial data to the server with ../pinData.R so it can be sourced by multiple apps


# Assessment Unit

First, some general housekeeping to ensure the applications can render as efficiently as possible. We will split up the AUs first by subbasin and then by Assessment region. 

## Split AUs for application easy rendering

First we need to take the spatial data we will use for the app and split it efficiently.

```{r split AUs for app}
# only run once
#source('preprocessingModules/splitAUbySubbasin.R')
```

### Pin spatial AU information to server for application use

This significantly speeds up app rendering.

```{r pin AU data}
#source('preprocessingModules/pinAUdataToServer.R')
```



## Assessment Unit Info from Last Cycle

The logical starting point is to take all the unique stations that need to be assessed (distinctSites_sf from above) and join AU information by StationID where possible before going to more computationally intensive methods. 

First, bring in the final Stations Table from the most recently completed IR cycle. For this example, we are sourcing the IR2022 final Stations Table (presented as a spatial object in a file geodatabase). We are also going to rename the friendly publication field names to a more standardized format that the automated assessment functions were built upon (read: we are changing field names to match previous versions of the Stations Table schema since the assessment functions were built on that data schema).


```{r 2022 final stations}

final2022 <- st_read('C:/HardDriveBackup/GIS/Assessment/2022IR_final/2022IR_GISData/va_ir22_wqms.gdb',
                     layer = 'va_ir22_wqms') %>% 
  st_drop_geometry() %>% # only need tabular data from here out
  # change names of ID305B columns to format required by automated methods
  rename(ID305B_1 = Assessment_Unit_ID_1, TYPE_1 = Station_Type_1,
         ID305B_2 = Assessment_Unit_ID_2, TYPE_2 = Station_Type_2,
         ID305B_3 = Assessment_Unit_ID_3, TYPE_3 = Station_Type_3,
         ID305B_4 = Assessment_Unit_ID_4, TYPE_4 = Station_Type_4,
         ID305B_5 = Assessment_Unit_ID_5, TYPE_5 = Station_Type_5,
         ID305B_6 = Assessment_Unit_ID_6, TYPE_6 = Station_Type_6,
         ID305B_7 = Assessment_Unit_ID_7, TYPE_7 = Station_Type_7,
         ID305B_8 = Assessment_Unit_ID_8, TYPE_8 = Station_Type_8,
         ID305B_9 = Assessment_Unit_ID_9, TYPE_9 = Station_Type_9,
         ID305B_10 = Assessment_Unit_ID_10, TYPE_10 = Station_Type_10)
```

Now we will join distinct sites to AU information to get all available data to start the assessment process. Note: Assessors may attribute stations to different VAHU6's compared to strictly where the site is located spatially to communicate that said stations (usually that lie close to VAHU6 border) are used to make assessment decisions about the designated VAHU6. For this reason, we use the VAHU6 designation from the previous assessment cycle over the VAHU6 retrieved from CEDS. If the station does not have a record in the previous assessment cycle Stations Table, the VAHU6 designation stored in CEDS is used. 

The last rows of the below chunk ensure that each station is only listed once in the resultant table. In previous assessment cycles, stations could be assessed for multiple waterbody types (e.g. riverine and lacustrine assessment uses). Since the assessment database was moved from MS Access to CEDS WQA, this duplication is no longer allowed and thus each station should only have one record.

```{r AU join}
distinctSites_AUall <- distinctSites_sf %>% 
  st_drop_geometry() %>%
  #rbind(missingSites_AU) %>%
  left_join(final2022 %>% 
              dplyr::select(-c(Latitude, Longitude)), # drop duplicate lat/lng fields to avoid join issues
            by = c('FDT_STA_ID' = 'Station_ID')) %>%
  dplyr::select(FDT_STA_ID : VAHU6.y) %>% # drop the last cycle's results, not important now
  mutate(VAHU6 = ifelse(is.na(VAHU6.y), as.character(VAHU6.x), as.character(VAHU6.y))) %>% # use last cycle's VAHU6 designation over CEDS designation by default if available
  dplyr::select(-c(VAHU6.x, VAHU6.y)) %>%
  group_by(FDT_STA_ID) %>%
  mutate(n = n()) %>% ungroup()

# Find any duplicates
View(filter(distinctSites_AUall, n >1)) # 0 rows, cool

# above n> 1 used to be stations that were riverine and lacustrine makes sense, these sites are being used for riverine and lacustrine assessment
# for IR2024 this should all be cleaned up bc new WQA CEDS rules, but always good to double check
```




Organize stations by whether or not they have AU data.

```{r AU haves and have nots}
distinctSites_AUtoDo <- filter(distinctSites_AUall, is.na(ID305B_1)) %>%
  st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
           remove = F, # don't remove these lat/lon cols from df
           crs = 4326)

# These sites are all good for automated assessment  (once we join WQS info from WQSlookup table)
distinctSites_AU <- filter(distinctSites_AUall, !is.na(ID305B_1)) 

# Quick QA: double check the math works out
nrow(distinctSites_AU) + nrow(distinctSites_AUtoDo) == nrow(distinctSites_AUall)
```


Next we need to side step to join 1a stations (from final IR2022 spatial layer) that really are in CEDS as 1A. **Note: this step isn't presented in the assessment bookdown bc above the level of understanding needed for most users but it is a very important step and still needs to be done in real life.**

```{r 1A fix}
distinctSites_AUtoDoFix <- filter(distinctSites_sf, FDT_STA_ID %in% distinctSites_AUtoDo$FDT_STA_ID) %>%
  st_drop_geometry() %>%
  left_join(final2022 %>% 
              dplyr::select(-c(Latitude, Longitude)) %>%  # drop duplicate lat/lng fields to avoid join issues
              mutate(STATION_ID = toupper(as.character(Station_ID))), 
                              by = c('FDT_STA_ID' = 'Station_ID')) %>%
  dplyr::select(FDT_STA_ID : VAHU6.y) %>% # drop the last cycle's results, not important now
  mutate(VAHU6 = ifelse(is.na(VAHU6.y), as.character(VAHU6.x), as.character(VAHU6.y))) %>% # use last cycle's VAHU6 designation over CEDS designation by default if available
  dplyr::select(-c(VAHU6.x, VAHU6.y)) %>%
  group_by(FDT_STA_ID) %>%
  mutate(n = n()) %>% ungroup()

# Add these back in if any sites identified
if(nrow(filter(distinctSites_AUtoDoFix, !is.na(ID305B_1))) > 0){
distinctSites_AUtoDoFixed <- filter(distinctSites_AUtoDoFix, !is.na(ID305B_1)) %>%
  st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
           remove = T, # don't remove these lat/lon cols from df
           crs = 4326)

distinctSites_AU <- #filter(distinctSites_AU,FDT_STA_ID %in% distinctSites_AUtoDoFixed$FDT_STA_ID) %>%
  rbind(distinctSites_AU, distinctSites_AUtoDoFixed)

distinctSites_AUtoDo <- filter(distinctSites_AUtoDo, ! FDT_STA_ID %in% distinctSites_AUtoDoFixed$FDT_STA_ID)
}


rm(distinctSites_AUtoDoFix); rm(distinctSites_AUtoDoFixed)

nrow(distinctSites_AU) + nrow(distinctSites_AUtoDo) == nrow(distinctSites_AUall)

```

### SKipped for 2024________________
Before we start spatially snapping things, let's double check none of these stations were already assigned an AU by the assessors during previous app review sessions.

```{r}
loadData <- function(outputDir) {
  # Read all the files into a list
  files <- list.files(outputDir, full.names = TRUE)
  # Concatenate all data together into one data.frame
  if(outputDir == 'WQSlookupTable'){
    data <- lapply(files, read_csv) 
    data <- do.call(rbind, data) %>%
      distinct(StationID, WQS_ID, .keep_all = T)
  } else {
    data <- lapply(files, read_csv) # read_csv produces parsing errors
    data <- do.call(rbind, data) %>%
      distinct(FDT_STA_ID, ID305B_1,  .keep_all = T)
  }
 
  data
}

userReviews <-  loadData("AUlookupTable") %>%
  group_by(FDT_STA_ID) %>%
  mutate(n = n()) %>%
  dplyr::select(FDT_STA_ID, ID305B_1, ID305B_2, ID305B_3, n, everything())

# first double check comments are the same and then remove duplicates due to additional row with ID305B_1=NA
userReviewsProblem <- filter(userReviews, n > 1 | is.na(ID305B_1)) %>%
  arrange(n, FDT_STA_ID) %>%
  filter(n > 1 & is.na(ID305B_1))

userReviews1 <- filter(userReviews, !(n > 1 & is.na(ID305B_1))) %>%
  group_by(FDT_STA_ID) %>%
  mutate(n = n()) %>%
  dplyr::select(FDT_STA_ID, ID305B_1, ID305B_2, ID305B_3, n, everything())
# now just write out to do last manual work
#write.csv(userReviews1, 'AUlookupTable/20210204_000000_AUlookup.csv') # Emma cleaned this up on 2/4/2021

userReviewsFixed <- read.csv('AUlookupTable/20210204_000000_AUlookup.csv') %>%
  rename('Buffer Distance'= 'Buffer.Distance',
         'Spatially Snapped' = 'Spatially.Snapped')

# there are still NA and n/a records in this version. I am going to leave the n/a because Rebecca reviewed those individually, but I don't trust the NA records aren't mistakes. I will throw those back to the assessors for review

userReviewsFixed <- filter(userReviewsFixed, !is.na(ID305B_1)) %>%
  dplyr::select(names(userReviews))

```

Now check to see if any of the sites missing AU info are actually in this userReviewsFixed object

```{r}
userFixed <- filter(userReviewsFixed, FDT_STA_ID %in% distinctSites_AUtoDo$FDT_STA_ID)

distinctSites_AUtoDoUserFixed <- filter(distinctSites_AUtoDo, FDT_STA_ID %in% userFixed$FDT_STA_ID) %>%
  left_join(dplyr::select(userFixed, FDT_STA_ID, ID305B_1, ID305B_2, # using ID305B_1 & 2 bc that's all I messed with manually
                          `Buffer Distance`, `Spatially Snapped`, Comments),
                                           by = 'FDT_STA_ID') %>%
  mutate(ID305B_1 = ID305B_1.y,
         ID305B_2 = ID305B_2.y) %>%
  dplyr::select(names(distinctSites_AUtoDo))

# add these to fixed AU's
distinctSites_AU <- #filter(distinctSites_AU,FDT_STA_ID %in% distinctSites_AUtoDoFixed$FDT_STA_ID) %>%
  rbind(distinctSites_AU, distinctSites_AUtoDoUserFixed)

# remove these from to do list
distinctSites_AUtoDo <- filter(distinctSites_AUtoDo, ! FDT_STA_ID %in% distinctSites_AUtoDoUserFixed$FDT_STA_ID)

rm(distinctSites_AUtoDoUserFixed); rm(userFixed); rm(userReviews1); rm(userReviews); rm(userReviewsFixed); rm(userReviewsProblem)

# double check no one lost
nrow(distinctSites_AU) + nrow(distinctSites_AUtoDo) == nrow(distinctSites_AUall)
```



###___END Skip____________________


## Recap: what has been done so far

At this point we have done all the necessary organization of the latest and greatest AU information for all potential stations we have to assess in the next IR cycle. Of these stations, we have identified which already have AU information from last cycle and which need to have an AU assigned before they can be run through the automated assessment scripts.

On to the process to spatially assign AU information to sites on out "to do" list.


## Spatially Join AU information

Since it is much faster to look for spatial joins by polygons compared to snapping to lines, we will run spatial joins by estuarine polys and reservoir layers first.

##### Estuarine Polygons AU

Find any sites that fall into an estuary AU polygon. This method is only applied to subbasins that intersect estuarine areas.
Removes any estuarine sites from the data frame of unique sites that need AU information.


```{r estuary methods AU}
source('preprocessingModules/AU_Poly.R')

# Bring in estuary layer
estuaryPolysAU <- st_read('C:/HardDriveBackup/GIS/Assessment/2022IR_final/2022IR_GISData/va_aus_estuarine.shp') %>%
   st_transform( 4326 ) %>%
   st_cast("MULTIPOLYGON") # special step for weird WKB error reading in geodatabase, never encountered before, fix from: https://github.com/r-spatial/sf/issues/427
  
estuaryPolysAUjoin <- polygonJoinAU(estuaryPolysAU, distinctSites_AUtoDo, estuaryTorF = T) %>%
  mutate(ID305B_1 = ID305B) %>%
  dplyr::select(names(distinctSites_AU)) %>%
  group_by(FDT_STA_ID) %>%
  mutate(n = n(),
         `Buffer Distance` = 'In polygon') %>%
  ungroup() 

rm(estuaryPolysAU) # clean up workspace
```

Add Estuary stations to distinctSites_AU.

```{r add estuary AU sites}
distinctSites_AU <- bind_rows(distinctSites_AU, estuaryPolysAUjoin %>% st_drop_geometry())
```


Remove stations that fell inside estuarine polygons from the 'to do' list.

```{r remove estuary poly sites AU}
distinctSites_AUtoDo <- filter(distinctSites_AUtoDo, ! FDT_STA_ID %in% estuaryPolysAUjoin$FDT_STA_ID)

nrow(distinctSites_AU) + nrow(distinctSites_AUtoDo) == nrow(distinctSites_AUall)
```


##### Lake Polygons

Find any sites that fall into a lake AU polygon. This method is applied to all subbasins.
Removes any lake sites from the data frame of unique sites that need AU information.


```{r lake methods AU}
source('preprocessingModules/AU_Poly.R')

# Bring in Lakes layer
lakesPolyAU <-  st_read('C:/HardDriveBackup/GIS/Assessment/2022IR_final/2022IR_GISData/va_aus_reservoir.shp') %>%
   st_transform( 4326 ) %>%
   st_cast("MULTIPOLYGON") # special step for weird WKB error reading in geodatabase, never encountered before, fix from: https://github.com/r-spatial/sf/issues/427

lakesPolysAUjoin <- polygonJoinAU(lakesPolyAU, distinctSites_AUtoDo, estuaryTorF = F)%>%
  mutate(ID305B_1 = ID305B,
         `Buffer Distance` = 'In polygon') %>%
  dplyr::select(names(distinctSites_AU)) %>%
  group_by(FDT_STA_ID) %>%
  mutate(n = n()) %>%
  ungroup() 

rm(lakesPolyAU) # clean up workspace
```

Add lake stations to distinctSites_AU.

```{r add lake AU sites}
distinctSites_AU <- bind_rows(distinctSites_AU, lakesPolysAUjoin %>% st_drop_geometry())
```



Remove stations that fell inside lake polygons from the 'to do' list.

```{r remove estuary poly sites}
distinctSites_AUtoDo <- filter(distinctSites_AUtoDo, ! FDT_STA_ID %in% lakesPolysAUjoin$FDT_STA_ID)

nrow(distinctSites_AU) + nrow(distinctSites_AUtoDo) == nrow(distinctSites_AUall)


rm(lakesPolysAUjoin);rm(estuaryPolysAUjoin)
```



### Spatially Join AU Lines

Now on to the more computationally heavy AU line snapping methods. First we will try to attach riverine AUs, and where stations remain we will try the estuarine lines AU snap.

##### Riverine Lines AU

Buffer all sites that don't fall into a polygon layer. The output will add a field called `Buffer Distance` to the distinctSites_AU to indicate distance required for snapping. This does not get transported to the data of record, but it is useful to keep for now for QA purposes. If more than one segment is found within a set buffer distance, that many rows will be attached to the WQStable with the identifying station name. It is up to the QA tool to help the user determine which of these AU's are correct and drop the other records.

Removes any riverine sites from the data frame of unique sites that need AU information.



```{r riverine methods AU}
source('snappingFunctions/snapPointToStreamNetwork.R')

riverineAU <- st_read('C:/HardDriveBackup/GIS/Assessment/2022IR_final/2022IR_GISData/va_aus_riverine.shp') %>%
     st_transform(102003)
  # 2/3/21 have to manually feed character proj4string bc doesn't recognize albers epsg 
  # st_transform('+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=37.5 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m no_defs')

  # Even the Edzer hack didn't work on riverine layers so exported draft riverine layer to shapefile for now. Hopefully final dataset comes as decent .gdb
  #st_read('C:/HardDriveBackup/GIS/Assessment/2020IR_draft/va_20ir_aus.gdb', layer = 'va_2020_aus_riverine' , 
              #            fid_column_name = "OBJECTID") %>%
  #st_transform(102003) %>% # forcing to albers from start bc such a huge layer   
  #st_cast("MULTILINESTRING") # special step for weird WKB error reading in geodatabase, never encountered before, fix from: https://github.com/r-spatial/sf/issues/427


snapTable <- snapAndOrganize(distinctSites_AUtoDo, 'FDT_STA_ID', riverineAU, 
                             bufferDistances = seq(20,80,by=20),  # buffering by 20m from 20 - 80 meters
                             tibble(StationID = character(), ID305B = character(), `Buffer Distance` = character()),
                             "ID305B")

#snapTable <- readRDS('preprocessingWorkflow/snapTable.RDS')

snapTable <- snapTable %>%
  left_join(distinctSites_AUtoDo, by = c('StationID' = 'FDT_STA_ID')) %>% # get station information
  rename('FDT_STA_ID' = 'StationID') %>%
  mutate(ID305B_1 = ID305B) %>%
  dplyr::select(names(distinctSites_AU), `Buffer Distance`) %>%
  group_by(FDT_STA_ID) %>%
  mutate(n = n()) %>%
  ungroup()
  
  
rm(riverineAU)
```

Add these sites to the sites with AU information.

```{r add to AU table}
distinctSites_AU <- bind_rows(distinctSites_AU , snapTable )
```

Remove stations that attached to riverine segments from the 'to do' list.

```{r remove riverine snapped AU sites}
distinctSites_AUtoDo <- filter(distinctSites_AUtoDo, ! FDT_STA_ID %in% snapTable$FDT_STA_ID)
```


We don't have estuarine lines AU information, so the sites that don't connect to any AU's at the max buffer distance will have to be sorted out by the assessors.

```{r}
distinctSites_AU <- distinctSites_AU %>%
  group_by(FDT_STA_ID) %>%
  mutate(n=n())
```


Make sure all stations from original distinct station list have some sort of record (blank or populated) in the distinctSites_AU dataset.

```{r double check no one lost AU}

# check everyone dealt with
nrow(distinctSites_AU) + nrow(distinctSites_AUtoDo) == nrow(distinctSites_AUall)

distinctSites_AU$FDT_STA_ID[!(distinctSites_sf$FDT_STA_ID %in% unique(distinctSites_AU$FDT_STA_ID))]

if(nrow(distinctSites_AUtoDo) == 0){rm(distinctSites_AUtoDo)}

#View(filter(distinctSites_sf, FDT_STA_ID %in% distinctSites_AU$FDT_STA_ID[!(distinctSites_sf$FDT_STA_ID %in% unique(distinctSites_AU$FDT_STA_ID))]))
```

And that none of the missing sites (sites that fall outside of the assessmentLayer) are not missing from the AU snapping process.

```{r missingSites got WQS}
missingSites$FDT_STA_ID[!(missingSites$FDT_STA_ID %in% unique(distinctSites_AU$FDT_STA_ID))]
```

Make sure buffer distances save right

```{r}
unique(distinctSites_AU$`Buffer Distance`)

distinctSites_AU$`Buffer Distance` <- as.character(distinctSites_AU$`Buffer Distance`)
```



Save your work!


```{r sites ready for app review}
write.csv(distinctSites_AU , 'data/preAnalyzedAUdata.csv', row.names = F)
```



### Intermediate app updates

Throughout the metadata process, there will be times when you need to grab the attributed data from the server, organize it, and push an app update. You first need to go to the server and grab all data attributed so far from the AUlookupTable and WQSlookupTable directories in the currently active application instance (see below for more info).

**This is not for when the assessors have completed all attribution, just for intermediate updates!**

Then you need to move these to the current working directory (manually using File Explorer) and parse the multiple csv's into a single csv.

```{r}
#source('organizeMetadataFromServer.R')
```


When you are ready to update the WQSlookup pinned data on the server, see 2.organizeMetadata/organizeStationMetadata.Rmd
you will need to copy over your cleaned up WQS information (e.g. 20230213_0000_WQSlookup.csv) to that working directory under data/metadataAttribution and run through the steps in 2.organizeMetadata/organizeStationMetadata.Rmd #WQS addition









# April 2023 Final data cleanup

```{r setup end, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(sf)
library(readxl)
library(lubridate)
#library(lwgeom) # used in assessmentLayerSubbasinJoin missingSites step
```

```{r connect to server end}
library(pins)
library(config)
library(purrr)
library(lubridate)

# Connect to server
conn <- config::get("connectionSettings") # get configuration settings

board_register_rsconnect(key = conn$CONNECT_API_KEY,  #Sys.getenv("CONNECT_API_KEY"),
                         server = conn$CONNECT_SERVER)#Sys.getenv("CONNECT_SERVER"))

```


Everything above happened iteratively prior to/at the data submission deadlines. Below archives what we did in April once we had a final dataset from Reid.

## Conventionals and Citmon Data

This is how it was done in March when only DEQ conventionals information was available:
draftMethodIterations/organizeConventionalsData.Rmd


In April 2023, Reid provided the conventionals data with all citmon/non agency data for that window in a single dataset. This final version of the method uses this dataset moving forward.

```{r}
conventionals <- readRDS('data/finalData/conv_citmon_nona_2017_22.RDS')

# pin final versions
# pin(conventionals, name = 'ejones/conventionals2024final',
#     description = 'Final IR2024 conventionals dataset, EVJ reorganized from Roger 3/03/2023 and Reid added Citmon/Nonagency 2017-2022 on 4/5/2023.',
#     board = 'rsconnect')

conventionals <- pin_get('ejones/conventionals2024final', board = 'rsconnect') 


# An object named conventionals is now in your environment to following the cleanup step above
```




#### distinctSites

Create a new object called distinctSites. This will be useful for keeping track of all anticipated sites for the new cycle. We will use this object to check which sites need WQS and which need AU. 

```{r conventionals missing WQS}
conventionals <- conventionals %>% 
  dplyr::select(FDT_STA_ID, GROUP_STA_ID, STA_DESC, Data_Source, Latitude, Longitude) %>% 
  distinct(FDT_STA_ID, .keep_all = T)

distinctSites <- conventionals
#dplyr::select(conventionals, -c(Latitude, Longitude)) # object to save unique sites for later, use standardized other lat/lng
```


Go to server and grab any station details that might be helpful.

```{r station spatial metadata}
WQMstationsView <- pin_get('ejones/WQM-Stations-View', board = 'rsconnect') %>% 
  dplyr::select(Sta_Id, Sta_Lv1_Code: Sta_Huc_Code)
WQMstations <- pin_get('ejones/WQM-Stations-Spatial', board = 'rsconnect') %>% 
  left_join(WQMstationsView, by = c('StationID' = 'Sta_Id'))
rm(WQMstationsView)

distinctSites1 <- left_join(distinctSites, WQMstations, by = c('FDT_STA_ID' = 'StationID')) %>% 
  distinct(FDT_STA_ID, .keep_all = T) %>% 
  mutate(STA_DESC = coalesce(STA_DESC, Sta_Desc),
         Latitude = coalesce(Latitude.x, Latitude.y),
         Longitude = coalesce(Longitude.x, Longitude.y)) %>% 
  dplyr::select(-c(Sta_Desc, Latitude.x, Latitude.y, Longitude.x, Longitude.y)) %>%  # remove duplicate column
  dplyr::select(FDT_STA_ID, GROUP_STA_ID, STA_DESC, Data_Source, Latitude, Longitude, everything())
  
# make sure we didn't lose anyone
distinctSites$FDT_STA_ID[! distinctSites$FDT_STA_ID %in% distinctSites1$FDT_STA_ID]

#rename objects to keep things organized
distinctSites <- distinctSites1
# clean up workspace 
rm(distinctSites1);rm(WQMstations)


# make sure all stations have a location, save any that don't for later citmon cleanup steps
missingCoordinates <- filter(distinctSites, is.na(Latitude) | is.na(Longitude)) 
  

# make a spatial dataset for later use
distinctSites_sf <- distinctSites %>% 
  filter(! FDT_STA_ID %in% missingCoordinates$FDT_STA_ID) %>% 
  st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
            remove = F, # don't remove these lat/lon cols from df
            crs = 4326)
# save for later just in case
#saveRDS(distinctSites_sf, './data/distinctSites_sf03072023.RDS')
```




Organize data to match distinctSites_sf format

```{r citmonNonagency spatial}
citmonNonAgency_sf <- filter(distinctSites_sf, is.na(ASSESS_REG)| is.na(US_L3NAME)) %>%  #citmonNonAgency %>% 
  dplyr::select(FDT_STA_ID, GROUP_STA_ID, STA_DESC, Data_Source,
                Latitude, Longitude) %>% 
  distinct(FDT_STA_ID, .keep_all = T) %>% 
   st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
           remove = F, # don't remove these lat/lon cols from df
           crs = 4326)
```


Need to add spatial metadata to make next steps work more efficiently.

```{r citmonNonagency spatial metadata}
#Bring in extra large ecoregion and assessment region files. By extra large we mean that they are continuous outside the state boundary such that stations that fall outside the state are not removed when spatially intersected
ecoregion4Large <- st_read('C:/HardDriveBackup/R/GitHub/pinData/data/GIS/VA_level4ecoregion_WGS84.shp')
vahu6 <- st_read('C:/HardDriveBackup/R/GitHub/pinData/data/GIS/VA_SUBWATERSHED_6TH_ORDER_STG.shp') # this version of vahu6 layer goes outside state boundary
subbasinConversion <- read_csv('C:/HardDriveBackup/R/GitHub/pinData/data/subbasinToVAHU6conversion.csv')

subbasinLayer <- st_read('../GIS/DEQ_VAHUSB_subbasins_EVJ.shp')  %>% 
  st_drop_geometry() %>% 
  rename('SUBBASIN' = 'SUBBASIN_1') %>% 
  distinct(subbasin, .keep_all = T) %>%  # don't need this by Assessment Region right now so simplify dataset to make next join cleaner
  dplyr::select(BASIN_NAME, BASIN_CODE, SUBBASIN, subbasin)

unrestrictedAssessmentRegionVAHU6Subbasin <- left_join(vahu6, subbasinConversion, by = c('VAHU6', 'VAHU5')) %>% 
  left_join(subbasinLayer, by = c('BASIN_CODE' = 'BASIN_CODE',
                                  'VAHUSB' = 'subbasin'))
#View(unrestrictedAssessmentRegionVAHU6Subbasin %>% st_drop_geometry())

# need this if running just citmon
#distinctSites_sf <- readRDS('./data/distinctSites_sf02232023.RDS')


# Add Level 3 and 4 Ecoregion Info to citmon and any conventionals stations taht don't have info
citmonNonAgency_sf1 <- rbind(citmonNonAgency_sf,
                             filter(conventionals, FDT_STA_ID %in% missingCoordinates$FDT_STA_ID) %>% 
                               st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
                                        remove = F, # don't remove these lat/lon cols from df
                                        crs = 4326)) %>% 
                             
  st_intersection(unrestrictedAssessmentRegionVAHU6Subbasin) %>% 
  st_intersection(dplyr::select(ecoregion4Large, US_L3CODE, US_L3NAME, US_L4CODE, US_L4NAME)) 

# add back in sites that could have been lost in spatial joins
citmonNonAgency_sf2 <- bind_rows(citmonNonAgency_sf1 %>% st_drop_geometry(), # drop geom so bind_rows will work instead of rbind
                                 filter(citmonNonAgency_sf, ! FDT_STA_ID %in% citmonNonAgency_sf1$FDT_STA_ID) %>% 
                                   st_drop_geometry()) %>% 
  # add back in DEQ stations that might not have made the spatial join cut
  bind_rows(filter(conventionals, FDT_STA_ID %in% missingCoordinates$FDT_STA_ID) %>% 
              filter( ! FDT_STA_ID %in% citmonNonAgency_sf1$FDT_STA_ID)) %>% 
  
  # change back to spatial object
   st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
           remove = F, # don't remove these lat/lon cols from df
           crs = 4326)

# make sure no duplicates
citmonNonAgency_sf2 %>% group_by(FDT_STA_ID) %>% mutate(n = n()) %>% filter(n > 1) %>% View()


# QA step
# filter(citmonNonAgency_sf, ! FDT_STA_ID %in% citmonNonAgency_sf2$FDT_STA_ID)
# mapview::mapview(filter(citmonNonAgency_sf, ! FDT_STA_ID %in% citmonNonAgency_sf1$FDT_STA_ID))+
#   mapview::mapview(ecoregion4Large)

distinctSites_sf <- bind_rows(distinctSites_sf %>% st_drop_geometry() %>% 
                                 filter(!FDT_STA_ID %in% citmonNonAgency_sf2$FDT_STA_ID),
                              citmonNonAgency_sf2 %>% st_drop_geometry() %>% 
                                dplyr::select(any_of(names(distinctSites_sf)))) %>%
  dplyr::select(names(distinctSites_sf %>% st_drop_geometry())) %>% # once you have the right column names, reorder to correct order
  st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
           remove = F, # don't remove these lat/lon cols from df
           crs = 4326) %>% 
  distinct(FDT_STA_ID, .keep_all = T) %>% 
 # mutate(GROUP_STA_ID = NA_character_) %>% #will need this when it comes time to do real citmon stuff
  dplyr::select(FDT_STA_ID, GROUP_STA_ID, everything())

# make sure no duplicates
distinctSites_sf %>% group_by(FDT_STA_ID) %>% mutate(n = n()) %>% dplyr::select(n, everything()) %>%  filter(n > 1) %>%   View()
# duplicates are all legit. These are colocated stations with citizen monitoring. KEEP THESE

# Clean up workspace
rm(list = c('ecoregion4Large','county', 'vahu6','subbasinConversion','unrestrictedAssessmentRegionVAHU6Subbasin',
            'citmonNonAgency_sf1', 'citmonNonAgency_sf2', 'path', 'subbasinLayer'))
```


### Remove Unwanted Stations

There are some stations that are deemed permanently inappropriate for assessment purposes that make it through the conventionals query. Remove these stations now so assessors don't need to see them in the attribution or further steps.

```{r staiton hit list}
stationHitList <- read_csv('data/stationHitList.csv')

distinctSites_sf <- filter(distinctSites_sf, ! FDT_STA_ID %in% stationHitList$`StationID to Remove`)


#saveRDS(distinctSites_sf, 'data/distinctSites_sf03072023.RDS')#data/distinctSites_sf_withCitmon02232023.RDS')
```





### Attach subbasin information

Assessment region is already included in the attached metadata thanks to the previous join to WQMstations. This information is important for processing each region through a loop for AU connection and WQS attachment. Subbasin information is important for WQS processing.

lwgeom needs sf >= 0.9.3, so this operation needs to happen in R 4.0.3. Unless you have changed any inputs prior to this point, just keep with 3.6.2 and read in data already processed.

```{r assessment and subbasin join}
#source('preprocessingModules/assessmentLayerSubbasinJoin.R') # lwgeom needs sf >= 0.9.3, so this operation needs to happen in R 4.0.3
distinctSites_sf <- readRDS('./data/distinctSites_sf_withCitmon03072023.RDS') #distinctSites_sf_withCitmon02232023.RDS')
distinctSitesToDo <- distinctSites_sf 
# keep a copy of original distinct sites to check that no one was lost at the end
``` 

**Note the VAHU6 data is derived spatially, so this is a good first step, but when there is human QAed VAHU6 data available, we will use that data instead. Sometimes assessors use stations outside a VAHU6 to assess said VAHU6.**


Save missingSites data for app. This is critical for those sites that fall out of assessment layer boundary that still need work. This will be brought in by app for assessor to use.

For 2024IR draft data, we didn't have any missingSites.

```{r missingSites for app}
#saveRDS(missingSites, 'data/missingSites.RDS')
```



### Pin Conventionals and Conventionals Distinct to server

2/13/2023- so this is confusing, where did conventionals_distinct come from? distinctSites_sf data structure doesn't match what is on server. also conventionals data was removed from environment. skipping for now as to not mess more things up

```{r}
# conventionals_distinct <- conventionals_distinct %>% 
#   filter(!is.na(Latitude) | !is.na(Longitude)) %>% 
#   st_as_sf(coords = c("Longitude", "Latitude"), 
#                remove = F, # don't remove these lat/lon cols from df
#                crs = 4326) %>% 
#   st_intersection(dplyr::select(basins, BASIN_CODE)) 
# pin('ejones/conventionals2024_distinctdraft', board = 'rsconnect')  # see below for fix coming this way at final stage

```

now pin to server


```{r}
# pin(distinctSites_sf, 'ejones/conventionals2024_distinctdraft', board = 'rsconnect',
#     description = 'Working IR2024 conventionals_distinct dataset based on conventionals final pull from Roger and 2017-2020 citmon/nonagency data from Reid that have FDT_STA_ID')  # see below for fix coming this way at final stage

```



# Attach WQS to Stations

Now we will work with our distinctSites_sf object to spatially join WQS information where it doesn't already exist. First, we need to establish which stations already have WQS_ID's so we don't duplicate efforts. This information is stored on the R server. 

```{r WQS info on server}
WQStableExisting <- pin_get('ejones/WQSlookup', board = 'rsconnect')

citmonWQS <- pin_get("ejones/citmonStationsWithWQSFinal", board = "rsconnect")
```


