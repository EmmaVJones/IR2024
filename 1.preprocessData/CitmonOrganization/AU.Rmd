---
title: "Untitled"
author: "Emma Jones"
date: "3/10/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(sf)
library(readxl)
library(lubridate)

library(pins)
library(config)
library(purrr)
library(lubridate)

# Connect to server
conn <- config::get(file = "C:/HardDriveBackup/R/GitHub/IR2024/1.preprocessData/config.yml", "connectionSettings") # get configuration settings

board_register_rsconnect(key = conn$CONNECT_API_KEY,  #Sys.getenv("CONNECT_API_KEY"),
                         server = conn$CONNECT_SERVER)#Sys.getenv("CONNECT_SERVER"))

```




Bring in the stations Reid is currently finishing the final data leveling on.

```{r citmonRead in}

noID <- read.csv('C:/HardDriveBackup/R/GitHub/IR2024/1.preprocessData/data/draftData/CM_Stations_21_22_NO_ID.csv') %>% 
  mutate_if(is.factor, as.character) %>% 
  #make a placeholder ID based on group name and station name for now
  mutate(FDT_STA_ID = paste(Data_Source, `Station.name`, sep = '.'))
withID <- read.csv('C:/HardDriveBackup/R/GitHub/IR2024/1.preprocessData/data/draftData/CM_Stations_21_22_With_ID.csv') %>% 
  mutate_if(is.factor, as.character) 

```







## Assessment Unit Info from Last Cycle

The logical starting point is to take all the unique stations that need to be assessed (distinctSites_sf from above) and join AU information by StationID where possible before going to more computationally intensive methods. 

First, bring in the final Stations Table from the most recently completed IR cycle. For this example, we are sourcing the IR2022 final Stations Table (presented as a spatial object in a file geodatabase). We are also going to rename the friendly publication field names to a more standardized format that the automated assessment functions were built upon (read: we are changing field names to match previous versions of the Stations Table schema since the assessment functions were built on that data schema).


```{r 2022 final stations}

final2022 <- st_read('C:/HardDriveBackup/GIS/Assessment/2022IR_final/2022IR_GISData/va_ir22_wqms.gdb',
                     layer = 'va_ir22_wqms') %>% 
  st_drop_geometry() %>% # only need tabular data from here out
  # change names of ID305B columns to format required by automated methods
  rename(ID305B_1 = Assessment_Unit_ID_1, TYPE_1 = Station_Type_1,
         ID305B_2 = Assessment_Unit_ID_2, TYPE_2 = Station_Type_2,
         ID305B_3 = Assessment_Unit_ID_3, TYPE_3 = Station_Type_3,
         ID305B_4 = Assessment_Unit_ID_4, TYPE_4 = Station_Type_4,
         ID305B_5 = Assessment_Unit_ID_5, TYPE_5 = Station_Type_5,
         ID305B_6 = Assessment_Unit_ID_6, TYPE_6 = Station_Type_6,
         ID305B_7 = Assessment_Unit_ID_7, TYPE_7 = Station_Type_7,
         ID305B_8 = Assessment_Unit_ID_8, TYPE_8 = Station_Type_8,
         ID305B_9 = Assessment_Unit_ID_9, TYPE_9 = Station_Type_9,
         ID305B_10 = Assessment_Unit_ID_10, TYPE_10 = Station_Type_10) %>% 
  dplyr::select(Station_ID:VAHU6, Comments:BasinName)
```


Now we will join distinct sites to AU information to get all available data to start the assessment process. Note: Assessors may attribute stations to different VAHU6's compared to strictly where the site is located spatially to communicate that said stations (usually that lie close to VAHU6 border) are used to make assessment decisions about the designated VAHU6. For this reason, we use the VAHU6 designation from the previous assessment cycle over the VAHU6 retrieved from CEDS. If the station does not have a record in the previous assessment cycle Stations Table, the VAHU6 designation stored in CEDS is used. 

The last rows of the below chunk ensure that each station is only listed once in the resultant table. In previous assessment cycles, stations could be assessed for multiple waterbody types (e.g. riverine and lacustrine assessment uses). Since the assessment database was moved from MS Access to CEDS WQA, this duplication is no longer allowed and thus each station should only have one record.

```{r AU join}
distinctSites_AUall <- withID %>% 
  left_join(final2022 %>% 
              dplyr::select(-c(Latitude, Longitude)), # drop duplicate lat/lng fields to avoid join issues
            by = c('FDT_STA_ID' = 'Station_ID')) 
```



Organize stations by whether or not they have AU data.

```{r AU haves and have nots}
distinctSites_AUtoDo <- filter(distinctSites_AUall, is.na(ID305B_1)) %>%
  bind_rows(noID) %>% 
  st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
           remove = F, # don't remove these lat/lon cols from df
           crs = 4326)

# These sites are all good for automated assessment  (once we join WQS info from WQSlookup table)
distinctSites_AU <- filter(distinctSites_AUall, !is.na(ID305B_1)) 

# Quick QA: double check the math works out
nrow(distinctSites_AU) + nrow(distinctSites_AUtoDo) == nrow(distinctSites_AUall) + nrow(noID)
```



Join in SUBBASIN information from WQS step

```{r}
distinctSites_sf <- read.csv('C:/HardDriveBackup/R/CitMonOrganization/distinctSites_sf20212022_ecobasin.csv') %>% 
  mutate(FDT_STA_ID = as.character(FDT_STA_ID))

distinctSites_sf$FDT_STA_ID %in% distinctSites_AUtoDo$FDT_STA_ID

distinctSites_AUtoDo <- left_join(distinctSites_AUtoDo, distinctSites_sf)#dplyr::select(distinctSites_sf, FDT_STA_ID, SUBBASIN, BASIN_CODE))


```




## Spatially Join AU information

Since it is much faster to look for spatial joins by polygons compared to snapping to lines, we will run spatial joins by estuarine polys and reservoir layers first.

##### Estuarine Polygons AU

Find any sites that fall into an estuary AU polygon. This method is only applied to subbasins that intersect estuarine areas.
Removes any estuarine sites from the data frame of unique sites that need AU information.


```{r estuary methods AU}
source('C:/HardDriveBackup/R/GitHub/IR2024/1.preprocessData/preprocessingModules/AU_Poly.R')

# Bring in estuary layer
estuaryPolysAU <- st_read('C:/HardDriveBackup/GIS/Assessment/2022IR_final/2022IR_GISData/va_aus_estuarine.shp') %>%
   st_transform( 4326 ) %>%
   st_cast("MULTIPOLYGON") # special step for weird WKB error reading in geodatabase, never encountered before, fix from: https://github.com/r-spatial/sf/issues/427
  
estuaryPolysAUjoin <- polygonJoinAU(estuaryPolysAU, distinctSites_AUtoDo, estuaryTorF = F) %>% # should be true but basin joining issues
  mutate(ID305B_1 = ID305B) %>%
  dplyr::select(names(distinctSites_AU)) %>%
  group_by(FDT_STA_ID) %>%
  mutate(n = n(),
         `Buffer Distance` = 'In polygon') %>%
  ungroup() 

rm(estuaryPolysAU) # clean up workspace
```

Add Estuary stations to distinctSites_AU.

```{r add estuary AU sites}
distinctSites_AU <- bind_rows(distinctSites_AU, estuaryPolysAUjoin %>% st_drop_geometry())
```


Remove stations that fell inside estuarine polygons from the 'to do' list.

```{r remove estuary poly sites AU}
distinctSites_AUtoDo <- filter(distinctSites_AUtoDo, ! FDT_STA_ID %in% estuaryPolysAUjoin$FDT_STA_ID)

nrow(distinctSites_AU) + nrow(distinctSites_AUtoDo) == nrow(distinctSites_AUall)+ nrow(noID)
```


##### Lake Polygons

Find any sites that fall into a lake AU polygon. This method is applied to all subbasins.
Removes any lake sites from the data frame of unique sites that need AU information.


```{r lake methods AU}
source('C:/HardDriveBackup/R/GitHub/IR2024/1.preprocessData/preprocessingModules/AU_Poly.R')

# Bring in Lakes layer
lakesPolyAU <-  st_read('C:/HardDriveBackup/GIS/Assessment/2022IR_final/2022IR_GISData/va_aus_reservoir.shp') %>%
   st_transform( 4326 ) %>%
   st_cast("MULTIPOLYGON") # special step for weird WKB error reading in geodatabase, never encountered before, fix from: https://github.com/r-spatial/sf/issues/427

lakesPolysAUjoin <- polygonJoinAU(lakesPolyAU, distinctSites_AUtoDo, estuaryTorF = F)%>%
  mutate(ID305B_1 = ID305B,
         `Buffer Distance` = 'In polygon') #%>%
  # dplyr::select(names(distinctSites_AU)) %>%
  # group_by(FDT_STA_ID) %>%
  # mutate(n = n()) %>%
  # ungroup() 

rm(lakesPolyAU) # clean up workspace
```

Add lake stations to distinctSites_AU.

```{r add lake AU sites}
distinctSites_AU <- bind_rows(distinctSites_AU, lakesPolysAUjoin %>% st_drop_geometry())
```



Remove stations that fell inside lake polygons from the 'to do' list.

```{r remove estuary poly sites}
distinctSites_AUtoDo <- filter(distinctSites_AUtoDo, ! FDT_STA_ID %in% lakesPolysAUjoin$FDT_STA_ID)

nrow(distinctSites_AU) + nrow(distinctSites_AUtoDo) == nrow(distinctSites_AUall)+ nrow(noID)


rm(lakesPolysAUjoin);rm(estuaryPolysAUjoin)
```



### Spatially Join AU Lines

Now on to the more computationally heavy AU line snapping methods. First we will try to attach riverine AUs, and where stations remain we will try the estuarine lines AU snap.

##### Riverine Lines AU

Buffer all sites that don't fall into a polygon layer. The output will add a field called `Buffer Distance` to the distinctSites_AU to indicate distance required for snapping. This does not get transported to the data of record, but it is useful to keep for now for QA purposes. If more than one segment is found within a set buffer distance, that many rows will be attached to the WQStable with the identifying station name. It is up to the QA tool to help the user determine which of these AU's are correct and drop the other records.

Removes any riverine sites from the data frame of unique sites that need AU information.



```{r riverine methods AU}
source('C:/HardDriveBackup/R/GitHub/IR2024/1.preprocessData/snappingFunctions/snapPointToStreamNetwork.R')

riverineAU <- st_read('C:/HardDriveBackup/GIS/Assessment/2022IR_final/2022IR_GISData/va_aus_riverine.shp') %>%
     st_transform(102003)
  # 2/3/21 have to manually feed character proj4string bc doesn't recognize albers epsg 
  # st_transform('+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=37.5 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m no_defs')

  # Even the Edzer hack didn't work on riverine layers so exported draft riverine layer to shapefile for now. Hopefully final dataset comes as decent .gdb
  #st_read('C:/HardDriveBackup/GIS/Assessment/2020IR_draft/va_20ir_aus.gdb', layer = 'va_2020_aus_riverine' , 
              #            fid_column_name = "OBJECTID") %>%
  #st_transform(102003) %>% # forcing to albers from start bc such a huge layer   
  #st_cast("MULTILINESTRING") # special step for weird WKB error reading in geodatabase, never encountered before, fix from: https://github.com/r-spatial/sf/issues/427


snapTable <- snapAndOrganize(distinctSites_AUtoDo[1,], 'FDT_STA_ID', riverineAU, 
                             bufferDistances = seq(20,80,by=20),  # buffering by 20m from 20 - 80 meters
                             tibble(StationID = character(), ID305B = character(), `Buffer Distance` = character()),
                             "ID305B")

#snapTable <- readRDS('preprocessingWorkflow/snapTable.RDS')

snapTable1 <- snapTable %>%
  left_join(distinctSites_AUtoDo, by = c('StationID' = 'FDT_STA_ID')) %>% # get station information
  rename('FDT_STA_ID' = 'StationID') %>%
  mutate(ID305B_1 = ID305B) %>%
  dplyr::select(any_of(names(distinctSites_AU %>% dplyr::select(-c(OBJECTID_1, OBJECTID.x, OBJECTID.y, ID305B)))), `Buffer Distance`) %>%
  group_by(FDT_STA_ID) %>%
  mutate(n = n()) %>%
  ungroup()
  
  
rm(riverineAU)
```

Add these sites to the sites with AU information.

```{r add to AU table}
distinctSites_AUtogether <- bind_rows(distinctSites_AU , snapTable1 ) %>% 
  dplyr::select(n, everything()) %>% 
  dplyr::select(n:SUBBASIN)
```

Remove stations that attached to riverine segments from the 'to do' list.

```{r remove riverine snapped AU sites}
distinctSites_AUtoDo <- filter(distinctSites_AUtoDo, ! FDT_STA_ID %in% snapTable1$FDT_STA_ID)
```


We don't have estuarine lines AU information, so the sites that don't connect to any AU's at the max buffer distance will have to be sorted out by the assessors.


Save for use later

```{r}
saveRDS(distinctSites_AUtogether, 'distinctSites_AUtogether.RDS')
```


