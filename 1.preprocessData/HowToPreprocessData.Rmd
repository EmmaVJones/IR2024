---
title: "Statewide Data Preprocessing"
author: "Emma Jones"
date: "7/22/2022"
output: html_document
---

After an update to lwgeom, this entire script needs to be run in R4.0.3 or you run into 'C Stack limit' errors at geospatial processes. Likely this has to do with updates to geospatial operations in underlying system dependencies with sf0.8 to sf 0.9, but not entirely sure.

All other scripts should be performed in R 3.6.2 to ensure they work on the server (which is still running GDAL 2.2X and PROJ4)


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(sf)
library(readxl)
library(lubridate)
library(lwgeom) # used in assessmentLayerSubbasinJoin missingSites step

```

This project is a outlines how to preprocess data for the manual WQS and AU attribution process. 

Each step will be written as individual modules where lengthy code to complete each step is stored elsewhere and sourced for use. This method of organizing the project helps with project maintenance and troubleshooting specific problem areas when they occur.

# Water Quality Standards Data Preprocessing

## WQS Split up

After each update of WQS layers, it is important to split these layers into bite size consumption for the review app such that we don't send unnecessary data to/from servers and speed up rending time. All snapping will be completed against the official WQS layers, but for app purposes, we will only bring in WQS in a selected subbasin.

Because most (all) DEQ subbasin layers incorrectly attribute the upper Potomac basin inside the Shenandoah in VRO, we will create our own layer to work from that fixes this error. 

```{r fix subbasin layer}
#source('./preprocessingModules/newDEQsubbasinLayerCreation.R') # only run once as needed
```

That really only needs to be done once such that 'GIS/DEQsubbasins_EVJ.shp' is created. This is the layer the splitWQSbySubbasin will use.

Now we take that newly created layer and use it to split up each of the riverine, lakes, and both types of estuary layers appropriately.


```{r splitWQS by subbasin for app}
#source('./preprocessingModules/splitWQSbySubbasin.R') # only run once as needed
```

And one last step to link basins to VAHU6 information for easier filtering inside the apps.

```{r build lookuptable for VAHU6 to subbasin}
#source('./preprocessingModules/VAHU6toSubbasin.R')
```


Now we can move on to actual data processing.




# Data Organization

Now we will create a list of all stations that are relevant to the IR 2024 assessment window. They are not all pulled from the same sources, so multiple datasets will be combined into a single object named distinctSites that we will then compare to existing WQS and AU information. Where stations in our distinctSites object lack either of these pieces of metadata, we will attribute them in this script.


### Conventionals Data

The largest dataset needed for these analyses is the conventionals dataset that Roger Stewart pulls every two years for each IR window. This is inclusive of data pulled from CEDS that should be used in the assessment. 

**Draft**
Since this process is written before the final conventionals data pull, we will use last cycle's conventionals pull and stations sampled since the close of the last IR window.

First bring in Roger's conventionals data pull for the 2022IR window. The data schema will be altered in 2.organizeMetadata to match Roland's new citmon database schema. For now, just use this as a list of stations sampled in the window for WQS and AU attribution.

```{r conventionals}
conventionalsRaw <- read_excel('data_old/CONVENTIONALS_20210316.xlsx') 

conventionals <-  mutate(conventionalsRaw, #Source_Sta_Id = as.character(NA),
                         Data_Source = 'DEQ') %>%
  dplyr::select(FDT_STA_ID, STA_DESC, Data_Source, FDT_DATE_TIME)
rm(conventionalsRaw)
```


Remove all stations that are outside the new IR window. We only care about stations that will be in the new cycle window, 2017-2022.

Create a new object called distinctSites. This will be useful for keeping track of all anticipated sites for the new cycle. We will use this object to check which sites need WQS and which need AU. 

```{r conventionals missing WQS}
conventionals <- filter(conventionals, year(FDT_DATE_TIME) >= 2017) %>% 
  distinct(FDT_STA_ID, .keep_all = T) %>% 
  dplyr::select(-FDT_DATE_TIME) 

distinctSites <- conventionals # object to save unique sites for later

rm(conventionals)# clean up workspace
```


Next pull stations sampled since the close of the 2022IR. The most efficient way to get this data is to use the monthly automated assessment pin on the R server. We will grab just the stations and locations and add that to our IR2022 conventionals list. 

```{r connect to server}
library(pins)
library(config)
library(purrr)
library(lubridate)

# Connect to server
conn <- config::get("connectionSettings") # get configuration settings

board_register_rsconnect(key = conn$CONNECT_API_KEY,  #Sys.getenv("CONNECT_API_KEY"),
                         server = conn$CONNECT_SERVER)#Sys.getenv("CONNECT_SERVER"))

```

```{r grab new stations}

#pull monthly assessment data
assessment <- pin_get("ejones/statewideResults", board = 'rsconnect')

# Pull station Table for each region, 
newStations <- assessment %>% 
  map_df('stationTable') %>% 
  dplyr::select(STATION_ID) %>% 
  mutate(Data_Source = 'DEQ')

# add newStations to distinctSites
distinctSites <- bind_rows(distinctSites,
                           dplyr::select(newStations, FDT_STA_ID = STATION_ID, Data_Source)) %>% 
  distinct(FDT_STA_ID, .keep_all = T) # drop duplicates that were added with newStations

rm(assessment); rm(newStations)
```

Go to server and grab any station details that might be helpful.

```{r station spatial metadata}
WQMstationsView <- pin_get('ejones/WQM-Stations-View', board = 'rsconnect') %>% 
  dplyr::select(Sta_Id, Sta_Lv1_Code: Sta_Huc_Code)
WQMstations <- pin_get('ejones/WQM-Stations-Spatial', board = 'rsconnect') %>% 
  left_join(WQMstationsView, by = c('StationID' = 'Sta_Id'))
rm(WQMstationsView)

distinctSites1 <- left_join(distinctSites, WQMstations, by = c('FDT_STA_ID' = 'StationID')) %>% 
  distinct(FDT_STA_ID, .keep_all = T) %>% 
  mutate(STA_DESC = coalesce(STA_DESC, Sta_Desc)) %>% 
  dplyr::select(-Sta_Desc) # remove duplicate column
  
# make sure we didn't lose anyone
distinctSites$FDT_STA_ID[! distinctSites$FDT_STA_ID %in% distinctSites1$FDT_STA_ID]

#rename objects to keep things organized
distinctSites <- distinctSites1
# clean up workspace 
rm(distinctSites1);rm(WQMstations)
```



### Other data: Citizen Monitoring and Non Agency

Stations pulled from other sources with Lat/Lng info could be run through the WQS steps if uploaded and reorganized here.













### Attach subbasin information

Assessment region is already included in the attached metadata thanks to the previous join to WQMstations. This information is important for processing each region through a loop for AU connection and WQS attachment. Subbasin information is important for WQS processing.

lwgeom needs sf >= 0.9.3, so this operation needs to happen in R 4.0.3. Unless you have changed any inputs prior to this point, just keep with 3.6.2 and read in data already processed.

```{r assessment and subbasin join}
#source('preprocessingModules/assessmentLayerSubbasinJoin.R') # lwgeom needs sf >= 0.9.3, so this operation needs to happen in R 4.0.3
distinctSites_sf <- readRDS('./data/distinctSites_sf.RDS')
distinctSitesToDo <- distinctSites_sf 
# keep a copy of original distinct sites to check that no one was lost at the end
``` 

**Note the VAHU6 data is derived spatially, so this is a good first step, but when there is human QAed VAHU6 data available, we will use that data instead. Sometimes assessors use stations outside a VAHU6 to assess said VAHU6.**


Save missingSites data for app. This is critical for those sites that fall out of assessment layer boundary that still need work. This will be brought in by app for assessor to use.

For 2024IR draft data, we didn't have any missingSites.

```{r missingSites for app}
#saveRDS(missingSites, 'data/missingSites.RDS')
```


# Attach WQS to Stations

Now we will work with our distinctSites_sf object to spatially join WQS information where it doesn't already exist. First, we need to establish which stations already have WQS_ID's so we don't duplicate efforts. This information is stored on the R server. 

```{r WQS info on server}
WQStableExisting <- pin_get('ejones/WQSlookup', board = 'rsconnect')
```


#### Aside, how to get WQS data from the server

To pull WQS information QAed in the metadata app and saved on the server, first go in to Connect and identify the app number of the current metadata application (under Regional Assessment Metadata Validation Tool- source versions). This information is critical to locating the area where info saved on the server. 

Next, in putty, see what info is available in that directory sudo  ls -l /app/rstudio-connect/apps/43/[sourceVersion]/WQSlookupTable where [sourceVersion] = the current build number from Connect.

Next, in filezilla, make sure the last version of this data pulled down is safely renamed by date to make sure grabbing new data doesnt overwrite old data. To do that, /home/aaejones/temp/ rename WQSlookupTable to WQSlookupTableFILEDATE (e.g. WQSlookupTable12152020).

Then in putty, copy whole directory and change all file permissions for download
sudo cp -arv /app/rstudio-connect/apps/43/[sourceVersion]/WQSlookupTable /home/aaejones/temp
sudo chmod -R 755 /home/aaejones/temp/WQSlookupTable

Then in filezilla, download the new WQSlookupTable directory to local system.

These are pulled directly from WQSLookupTable directories and can contain issues
```{r}
#source('./preprocessingModules/pullWQSID.R') # this pulls all stations in WQSLookupTable directory, which could include duplicates
```

#### End WQS data from server aside

## Remove stations that have WQS information from "to do" list

Identify real number of sites that need spatial joining.

```{r find where spatial joins have already happened}
distinctSitesToDoWQS <- filter(distinctSitesToDo, ! FDT_STA_ID  %in% WQStableExisting$StationID)
```


#### Spatially Join WQS

Since transitioning the WQS storage from local (individual assessor files) to a centralized system (on the R server for multiple program uses), the number of stations that require WQS attribution should decrease significantly each IR cycle. To attribute each station to WQS information, we first need to do all spatial joins to new WQS layer to get appropriate UID information, then we can send that information to an interactive application for humans to manually verify.

Here is the table used to store link information from stations to appropriate WQS.

```{r WQStable}
WQStable <- tibble(StationID = NA, WQS_ID = NA)
```


Since it is much faster to look for spatial joins by polygons compared to snapping to lines, we will run spatial joins by estuarine polys and reservoir layers first.

##### Estuarine Polygons

Find any sites that fall into an estuary WQS polygon. This method is only applied to subbasins that intersect estuarine areas. This process also removes any estuarine sites from the data frame of unique sites that need WQS information (distinctSitesToDoWQS).

We will bring in (source) a custom function that runs the analysis for us, feed it our latest WQS information, and let the function run across all input stations.

```{r estuary methods}
source('preprocessingModules/WQS_estuaryPoly.R')

# Bring in estuary layer
estuarinePolys <- st_read('../GIS/draft_WQS_shapefiles_2022/estuarinepolygons_draft2022.shp', 
                          fid_column_name = "OBJECTID") %>%
  st_transform(4326)
  
WQStable <- estuaryPolygonJoin(estuarinePolys, distinctSitesToDoWQS, WQStable)

rm(estuarinePolys) # clean up workspace
```

Remove stations that fell inside estuarine polygons from the 'to do' list.

```{r remove estuary poly sites}
distinctSitesToDoWQS <- filter(distinctSitesToDoWQS, ! FDT_STA_ID %in% WQStable$StationID)
```


##### Lake Polygons

Find any sites that fall into a lake WQS polygon. This method is applied to all subbasins at once as it is a simple spatial join. 
The next step removes any lake sites from the data frame of unique sites that need WQS information (distinctSitesToDoWQS).


```{r lake methods}
source('preprocessingModules/WQS_lakePoly.R')

lakesPoly <- st_read('../GIS/draft_WQS_shapefiles_2022/lakes_reservoirs_draft2022.shp',
                     fid_column_name = "OBJECTID") %>%
  st_transform(4326)

WQStable <- lakePolygonJoin(lakesPoly, distinctSitesToDoWQS, WQStable)

rm(lakesPoly) # clean up workspace
```

Remove stations that fell inside lake polygons from the 'to do' list.

```{r remove estuary poly sites}
distinctSitesToDoWQS <- filter(distinctSitesToDoWQS, ! FDT_STA_ID %in% WQStable$StationID)
```



### Spatially Join WQS Lines

Now on to the more computationally heavy WQS line snapping methods. First we will try to attach riverine WQS, and where stations remain we will try the estuarine WQS snap.

##### Riverine Lines

To do this join, we will buffer all sites that don't fall into a polygon layer by a set sequence of distances. The output will add a field called `Buffer Distance` to the WQStable to indicate distance required for snapping. This does not get transported to the data of record, but it is useful to keep for now for QA purposes. If more than one segment is found within a set buffer distance, then many rows will be attached to the WQStable with the single identifying station name. It is up to the QA tool to help the user determine which of these UID's are correct and drop the other records.

We then remove any riverine sites from the data frame of unique sites that need WQS information (distinctSitesToDoWQS).

```{r riverine methods}
source('snappingFunctions/snapWQS.R')

riverine <- st_read('../GIS/draft_WQS_shapefiles_2022/riverine_draft2022.shp',
                    fid_column_name = "OBJECTID") #%>% 
  #st_transform(102003) # forcing to albers from start bc such a huge layer
  #st_transform('+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=37.5 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m no_defs')
# 2/3/21 have to manually feed character proj4string bc doesn't recognize albers epsg 

WQStable <- snapAndOrganizeWQS(distinctSitesToDoWQS, 'FDT_STA_ID', riverine, 
                               bufferDistances = seq(20,80,by=20),  # buffering by 20m from 20 - 80 meters
                               WQStable)
#saveRDS(WQStable, 'data/WQStable.RDS')

rm(riverine)
```

Remove stations that attached to riverine segments from the 'to do' list.

```{r remove estuary poly sites}
distinctSitesToDoWQS <- filter(distinctSitesToDoWQS, ! FDT_STA_ID %in% WQStable$StationID)
```


We can use this one last opportunity to test stations that didn't connect to the riverine WQS against the estuarine lines WQS as a one last hope of attributing some WQS information. We will take all stations from the WQStable that didn't snap to any WQS segments (`Buffer Distance` =='No connections within 80 m') and add those back in to our distinctSitesToDoWQS list to try to snap them to the estuarine lines spatial data. 

```{r no riverine snaps}
distinctSitesToDoWQS <- filter(WQStable, `Buffer Distance` =='No connections within 80 m') %>%
  left_join(dplyr::select(distinctSites_sf, FDT_STA_ID, Latitude, Longitude, SUBBASIN) %>%
              rename('StationID'= 'FDT_STA_ID'), by='StationID') %>%
  dplyr::select(-c(geometry)) %>%
  st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
           remove = T, # don't remove these lat/lon cols from df
           crs = 4326)
                            
```



##### Estuarine Lines

If a site doesn't attach to a riverine segment, our last step is to try to attach estuary line segments before throwing an empty site to the users for the wild west of manual QA. Only send sites that could be in estuarine subbasin to this function to not waste time.
Removes any estuary lines sites from the data frame of unique sites that need WQS information.

```{r estuarine lines methods}
source('snappingFunctions/snapWQS.R')

estuarineLines <- st_read('../GIS/draft_WQS_shapefiles_2022/estuarinelines_draft2022.shp' , fid_column_name = "OBJECTID") #%>%
  #st_transform(102003) # forcing to albers from start bc such a huge layer
  #st_transform(4326)

# Only send sites to function that could be in estuarine environment
WQStable <- snapAndOrganizeWQS(filter(distinctSitesToDoWQS, SUBBASIN %in% c("Potomac River",
                                                                            "Rappahannock River", 
                                                                            "Atlantic Ocean Coastal",
                                                                            "Chesapeake Bay Tributaries",
                                                                            "Chesapeake Bay - Mainstem",
                                                                            "James River - Lower",  
                                                                            "Appomattox River",
                                                                            "Chowan River", 
                                                                            "Atlantic Ocean - South" ,
                                                                            "Dismal Swamp/Albemarle Sound")),
                               'StationID', estuarineLines, 
                               bufferDistances = seq(20,80,by=20),  # buffering by 20m from 20 - 80 meters
                               WQStable)
saveRDS(WQStable, 'WQStable2.RDS')
rm(estuarineLines)
```

Remove stations that attached to estuarine segments from the 'to do' list.

```{r remove estuary poly sites}
distinctSitesToDoWQS <- filter(distinctSitesToDoWQS, ! StationID %in% WQStable$StationID)
```


Double check no stations were lost in these processes. 

```{r double check no one lost}
#Make sure all stations from original distinct station list have some sort of record (blank or populated) int the WQStable.

distinctSitesToDoWQS <- filter(distinctSitesToDo, ! FDT_STA_ID  %in% WQStableExisting$StationID)
distinctSitesToDoWQS$FDT_STA_ID[!(distinctSitesToDoWQS$FDT_STA_ID %in% unique(WQStable$StationID))]
```

And that none of the missing sites (sites that fall outside of the assessmentLayer) are not missing from the WQS snapping process.

```{r missingSites got WQS}
missingSites <- readRDS('data/missingSites.RDS')
missingSites$FDT_STA_ID[!(missingSites$FDT_STA_ID %in% unique(WQStable$StationID))]
```


### Assign something to WQS_ID so sites will not fall through the cracks when application filtering occurs

We don't want to give everyone all the stations that didn't snap to something, so we need to at least partially assign a WQS_ID so the stations get into the correct subbasin on initial filter.

If a station snapped to nothing, we will assigning it a RL WQS_ID and subbasin it falls into by default.

```{r blank WQS_ID partially filled in}
WQStableMissing <- filter(WQStable, is.na(WQS_ID)) %>%
  # drop from list if actually fixed by snap to another segment
  filter(! StationID %in% filter(WQStable, str_extract(WQS_ID, "^.{2}") %in% c('EL','LP','EP'))$StationID) %>%
  left_join(dplyr::select(distinctSites_sf, FDT_STA_ID, BASIN_CODE) %>%
              st_drop_geometry(), by = c('StationID' = 'FDT_STA_ID')) %>%
  # some fixes for missing basin codes so they will match proper naming conventions for filtering
  mutate(BASIN_CODE1 = case_when(is.na(BASIN_CODE) ~ str_pad(
    ifelse(grepl('-', str_extract(StationID, "^.{2}")), str_extract(StationID, "^.{1}"), str_extract(StationID, "^.{2}")), 
    width = 2, side = 'left', pad = '0'),
    TRUE ~ as.character(BASIN_CODE)),
    BASIN_CODE2 = str_pad(BASIN_CODE1, width = 2, side = 'left', pad = '0'),
    WQS_ID = paste0('RL_', BASIN_CODE2,'_NA')) %>%
  dplyr::select(-c(BASIN_CODE, BASIN_CODE1, BASIN_CODE2))

WQStable <- filter(WQStable, !is.na(WQS_ID)) %>% 
  filter(! StationID %in% WQStableMissing$StationID) %>%
  bind_rows(WQStableMissing)
```





### QA Spatial Attribution Process

See how many sites snapped to too many WQS segments

```{r snapCheck}
tooMany <- snapCheck(WQStable)

# quick snap check previous WQS attributed data from assessors bc some issues have been noticed
tooMany_WQStableExisting <- snapCheck(WQStableExisting) %>%
  dplyr::select(-Comments) # delete this so we can sent it back to user to be fixed

WQStable <- bind_rows(WQStable, tooMany_WQStableExisting)


fine <- filter(WQStable, ! (StationID %in% tooMany$StationID)) %>% 
  filter(`Buffer Distance` %in% c(NA,  "20 m", "40 m", "60 m" ,"80 m"))
none <- filter(WQStable, ! StationID %in% tooMany$StationID) %>%
  filter(`Buffer Distance` == "No connections within 80 m")

# quick stats 
(nrow(fine) / nrow(distinctSites)) * 100 # ~ 74% snapped to one segment
(length(unique(tooMany$StationID)) / nrow(distinctSites)) * 100 # ~ 23% need extra manual review
(nrow(none) / nrow(distinctSites)) * 100 # ~ 4% snapped to one segment

rm(fine);rm(none);rm(tooMany)
```

skipped some stuff. double check when have citmon etc if needed


Save WQStable for manual review in application. 
```{r save WQS for app review}
saveRDS(WQStable, './data/WQStable10182022.RDS')

# clean up workspace
rm(list = c('distinctSitesToDoWQS','WQStable', 'WQStableExisting','WQStableMissing'))
```


Pin spatial data to the server with ../pinData.R so it can be sourced by multiple apps


# Assessment Unit

First, some general housekeeping to ensure the applications can render as efficiently as possible. We will split up the AUs first by subbasin and then by Assessment region. 

## Split AUs for application easy rendering

First we need to take the spatial data we will use for the app and split it efficiently.

```{r split AUs for app}
# only run once
#source('preprocessingModules/splitAUbySubbasin.R')
```

### Pin spatial AU information to server for application use

This significantly speeds up app rendering.

```{r pin AU data}
#source('preprocessingModules/pinAUdataToServer.R')
```



## Assessment Unit Info from Last Cycle

The logical starting point is to take all the unique stations that need to be assessed (distinctSites_sf from above) and join AU information by StationID where possible before going to more computationally intensive methods. 

First, bring in the final Stations Table from the most recently completed IR cycle. For this example, we are sourcing the IR2022 final Stations Table (presented as a spatial object in a file geodatabase). We are also going to rename the friendly publication field names to a more standardized format that the automated assessment functions were built upon (read: we are changing field names to match previous versions of the Stations Table schema since the assessment functions were built on that data schema).


```{r 2022 final stations}

final2022 <- st_read('C:/HardDriveBackup/GIS/Assessment/2022IR_final/2022IR_GISData/va_ir22_wqms.gdb',
                     layer = 'va_ir22_wqms') %>% 
  st_drop_geometry() %>% # only need tabular data from here out
  # change names of ID305B columns to format required by automated methods
  rename(ID305B_1 = Assessment_Unit_ID_1, TYPE_1 = Station_Type_1,
         ID305B_2 = Assessment_Unit_ID_2, TYPE_2 = Station_Type_2,
         ID305B_3 = Assessment_Unit_ID_3, TYPE_3 = Station_Type_3,
         ID305B_4 = Assessment_Unit_ID_4, TYPE_4 = Station_Type_4,
         ID305B_5 = Assessment_Unit_ID_5, TYPE_5 = Station_Type_5,
         ID305B_6 = Assessment_Unit_ID_6, TYPE_6 = Station_Type_6,
         ID305B_7 = Assessment_Unit_ID_7, TYPE_7 = Station_Type_7,
         ID305B_8 = Assessment_Unit_ID_8, TYPE_8 = Station_Type_8,
         ID305B_9 = Assessment_Unit_ID_9, TYPE_9 = Station_Type_9,
         ID305B_10 = Assessment_Unit_ID_10, TYPE_10 = Station_Type_10)
```

Now we will join distinct sites to AU information to get all available data to start the assessment process. Note: Assessors may attribute stations to different VAHU6's compared to strictly where the site is located spatially to communicate that said stations (usually that lie close to VAHU6 border) are used to make assessment decisions about the designated VAHU6. For this reason, we use the VAHU6 designation from the previous assessment cycle over the VAHU6 retrieved from CEDS. If the station does not have a record in the previous assessment cycle Stations Table, the VAHU6 designation stored in CEDS is used. 

The last rows of the below chunk ensure that each station is only listed once in the resultant table. In previous assessment cycles, stations could be assessed for multiple waterbody types (e.g. riverine and lacustrine assessment uses). Since the assessment database was moved from MS Access to CEDS WQA, this duplication is no longer allowed and thus each station should only have one record.

```{r AU join}
distinctSites_AUall <- distinctSites_sf %>% 
  st_drop_geometry() %>%
  #rbind(missingSites_AU) %>%
  left_join(final2022 %>% 
              dplyr::select(-c(Latitude, Longitude)), # drop duplicate lat/lng fields to avoid join issues
            by = c('FDT_STA_ID' = 'Station_ID')) %>%
  dplyr::select(FDT_STA_ID : VAHU6.y) %>% # drop the last cycle's results, not important now
  mutate(VAHU6 = ifelse(is.na(VAHU6.y), as.character(VAHU6.x), as.character(VAHU6.y))) %>% # use last cycle's VAHU6 designation over CEDS designation by default if available
  dplyr::select(-c(VAHU6.x, VAHU6.y)) %>%
  group_by(FDT_STA_ID) %>%
  mutate(n = n()) %>% ungroup()

# Find any duplicates
View(filter(distinctSites_AUall, n >1)) # 0 rows, cool

# above n> 1 used to be stations that were riverine and lacustrine makes sense, these sites are being used for riverine and lacustrine assessment
# for IR2024 this should all be cleaned up bc new WQA CEDS rules, but always good to double check
```




Organize stations by whether or not they have AU data.

```{r AU haves and have nots}
distinctSites_AUtoDo <- filter(distinctSites_AUall, is.na(ID305B_1)) %>%
  st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
           remove = F, # don't remove these lat/lon cols from df
           crs = 4326)

# These sites are all good for automated assessment  (once we join WQS info from WQSlookup table)
distinctSites_AU <- filter(distinctSites_AUall, !is.na(ID305B_1)) 

# Quick QA: double check the math works out
nrow(distinctSites_AU) + nrow(distinctSites_AUtoDo) == nrow(distinctSites_AUall)
```


Next we need to side step to join 1a stations (from final IR2022 spatial layer) that really are in CEDS as 1A. **Note: this step isn't presented in the assessment bookdown bc above the level of understanding needed for most users but it is a very important step and still needs to be done in real life.**

```{r 1A fix}
distinctSites_AUtoDoFix <- filter(distinctSites_sf, FDT_STA_ID %in% distinctSites_AUtoDo$FDT_STA_ID) %>%
  st_drop_geometry() %>%
  left_join(final2022 %>% 
              dplyr::select(-c(Latitude, Longitude)) %>%  # drop duplicate lat/lng fields to avoid join issues
              mutate(STATION_ID = toupper(as.character(Station_ID))), 
                              by = c('FDT_STA_ID' = 'Station_ID')) %>%
  dplyr::select(FDT_STA_ID : VAHU6.y) %>% # drop the last cycle's results, not important now
  mutate(VAHU6 = ifelse(is.na(VAHU6.y), as.character(VAHU6.x), as.character(VAHU6.y))) %>% # use last cycle's VAHU6 designation over CEDS designation by default if available
  dplyr::select(-c(VAHU6.x, VAHU6.y)) %>%
  group_by(FDT_STA_ID) %>%
  mutate(n = n()) %>% ungroup()

# Add these back in if any sites identified
if(nrow(filter(distinctSites_AUtoDoFix, !is.na(ID305B_1))) > 0){
distinctSites_AUtoDoFixed <- filter(distinctSites_AUtoDoFix, !is.na(ID305B_1)) %>%
  st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
           remove = T, # don't remove these lat/lon cols from df
           crs = 4326)

distinctSites_AU <- #filter(distinctSites_AU,FDT_STA_ID %in% distinctSites_AUtoDoFixed$FDT_STA_ID) %>%
  rbind(distinctSites_AU, distinctSites_AUtoDoFixed)

distinctSites_AUtoDo <- filter(distinctSites_AUtoDo, ! FDT_STA_ID %in% distinctSites_AUtoDoFixed$FDT_STA_ID)
}


rm(distinctSites_AUtoDoFix); rm(distinctSites_AUtoDoFixed)

nrow(distinctSites_AU) + nrow(distinctSites_AUtoDo) == nrow(distinctSites_AUall)

```

### SKipped for 2024________________
Before we start spatially snapping things, let's double check none of these stations were already assigned an AU by the assessors during previous app review sessions.

```{r}
loadData <- function(outputDir) {
  # Read all the files into a list
  files <- list.files(outputDir, full.names = TRUE)
  # Concatenate all data together into one data.frame
  if(outputDir == 'WQSlookupTable'){
    data <- lapply(files, read_csv) 
    data <- do.call(rbind, data) %>%
      distinct(StationID, WQS_ID, .keep_all = T)
  } else {
    data <- lapply(files, read_csv) # read_csv produces parsing errors
    data <- do.call(rbind, data) %>%
      distinct(FDT_STA_ID, ID305B_1,  .keep_all = T)
  }
 
  data
}

userReviews <-  loadData("AUlookupTable") %>%
  group_by(FDT_STA_ID) %>%
  mutate(n = n()) %>%
  dplyr::select(FDT_STA_ID, ID305B_1, ID305B_2, ID305B_3, n, everything())

# first double check comments are the same and then remove duplicates due to additional row with ID305B_1=NA
userReviewsProblem <- filter(userReviews, n > 1 | is.na(ID305B_1)) %>%
  arrange(n, FDT_STA_ID) %>%
  filter(n > 1 & is.na(ID305B_1))

userReviews1 <- filter(userReviews, !(n > 1 & is.na(ID305B_1))) %>%
  group_by(FDT_STA_ID) %>%
  mutate(n = n()) %>%
  dplyr::select(FDT_STA_ID, ID305B_1, ID305B_2, ID305B_3, n, everything())
# now just write out to do last manual work
#write.csv(userReviews1, 'AUlookupTable/20210204_000000_AUlookup.csv') # Emma cleaned this up on 2/4/2021

userReviewsFixed <- read.csv('AUlookupTable/20210204_000000_AUlookup.csv') %>%
  rename('Buffer Distance'= 'Buffer.Distance',
         'Spatially Snapped' = 'Spatially.Snapped')

# there are still NA and n/a records in this version. I am going to leave the n/a because Rebecca reviewed those individually, but I don't trust the NA records aren't mistakes. I will throw those back to the assessors for review

userReviewsFixed <- filter(userReviewsFixed, !is.na(ID305B_1)) %>%
  dplyr::select(names(userReviews))

```

Now check to see if any of the sites missing AU info are actually in this userReviewsFixed object

```{r}
userFixed <- filter(userReviewsFixed, FDT_STA_ID %in% distinctSites_AUtoDo$FDT_STA_ID)

distinctSites_AUtoDoUserFixed <- filter(distinctSites_AUtoDo, FDT_STA_ID %in% userFixed$FDT_STA_ID) %>%
  left_join(dplyr::select(userFixed, FDT_STA_ID, ID305B_1, ID305B_2, # using ID305B_1 & 2 bc that's all I messed with manually
                          `Buffer Distance`, `Spatially Snapped`, Comments),
                                           by = 'FDT_STA_ID') %>%
  mutate(ID305B_1 = ID305B_1.y,
         ID305B_2 = ID305B_2.y) %>%
  dplyr::select(names(distinctSites_AUtoDo))

# add these to fixed AU's
distinctSites_AU <- #filter(distinctSites_AU,FDT_STA_ID %in% distinctSites_AUtoDoFixed$FDT_STA_ID) %>%
  rbind(distinctSites_AU, distinctSites_AUtoDoUserFixed)

# remove these from to do list
distinctSites_AUtoDo <- filter(distinctSites_AUtoDo, ! FDT_STA_ID %in% distinctSites_AUtoDoUserFixed$FDT_STA_ID)

rm(distinctSites_AUtoDoUserFixed); rm(userFixed); rm(userReviews1); rm(userReviews); rm(userReviewsFixed); rm(userReviewsProblem)

# double check no one lost
nrow(distinctSites_AU) + nrow(distinctSites_AUtoDo) == nrow(distinctSites_AUall)
```



###___END Skip____________________


## Recap: what has been done so far

At this point we have done all the necessary organization of the latest and greatest AU information for all potential stations we have to assess in the next IR cycle. Of these stations, we have identified which already have AU information from last cycle and which need to have an AU assigned before they can be run through the automated assessment scripts.

On to the process to spatially assign AU information to sites on out "to do" list.


## Spatially Join AU information

Since it is much faster to look for spatial joins by polygons compared to snapping to lines, we will run spatial joins by estuarine polys and reservoir layers first.

##### Estuarine Polygons AU

Find any sites that fall into an estuary AU polygon. This method is only applied to subbasins that intersect estuarine areas.
Removes any estuarine sites from the data frame of unique sites that need AU information.


```{r estuary methods AU}
source('preprocessingModules/AU_Poly.R')

# Bring in estuary layer
estuaryPolysAU <- st_read('C:/HardDriveBackup/GIS/Assessment/2022IR_final/2022IR_GISData/va_aus_estuarine.shp') %>%
   st_transform( 4326 ) %>%
   st_cast("MULTIPOLYGON") # special step for weird WKB error reading in geodatabase, never encountered before, fix from: https://github.com/r-spatial/sf/issues/427
  
estuaryPolysAUjoin <- polygonJoinAU(estuaryPolysAU, distinctSites_AUtoDo, estuaryTorF = T) %>%
  mutate(ID305B_1 = ID305B) %>%
  dplyr::select(names(distinctSites_AU)) %>%
  group_by(FDT_STA_ID) %>%
  mutate(n = n(),
         `Buffer Distance` = 'In polygon') %>%
  ungroup() 

rm(estuaryPolysAU) # clean up workspace
```

Add Estuary stations to distinctSites_AU.

```{r add estuary AU sites}
distinctSites_AU <- bind_rows(distinctSites_AU, estuaryPolysAUjoin %>% st_drop_geometry())
```


Remove stations that fell inside estuarine polygons from the 'to do' list.

```{r remove estuary poly sites AU}
distinctSites_AUtoDo <- filter(distinctSites_AUtoDo, ! FDT_STA_ID %in% estuaryPolysAUjoin$FDT_STA_ID)

nrow(distinctSites_AU) + nrow(distinctSites_AUtoDo) == nrow(distinctSites_AUall)
```


##### Lake Polygons

Find any sites that fall into a lake AU polygon. This method is applied to all subbasins.
Removes any lake sites from the data frame of unique sites that need AU information.


```{r lake methods AU}
source('preprocessingModules/AU_Poly.R')

# Bring in Lakes layer
lakesPolyAU <-  st_read('C:/HardDriveBackup/GIS/Assessment/2022IR_final/2022IR_GISData/va_aus_reservoir.shp') %>%
   st_transform( 4326 ) %>%
   st_cast("MULTIPOLYGON") # special step for weird WKB error reading in geodatabase, never encountered before, fix from: https://github.com/r-spatial/sf/issues/427

lakesPolysAUjoin <- polygonJoinAU(lakesPolyAU, distinctSites_AUtoDo, estuaryTorF = F)%>%
  mutate(ID305B_1 = ID305B,
         `Buffer Distance` = 'In polygon') %>%
  dplyr::select(names(distinctSites_AU)) %>%
  group_by(FDT_STA_ID) %>%
  mutate(n = n()) %>%
  ungroup() 

rm(lakesPolyAU) # clean up workspace
```

Add lake stations to distinctSites_AU.

```{r add lake AU sites}
distinctSites_AU <- bind_rows(distinctSites_AU, lakesPolysAUjoin %>% st_drop_geometry())
```



Remove stations that fell inside lake polygons from the 'to do' list.

```{r remove estuary poly sites}
distinctSites_AUtoDo <- filter(distinctSites_AUtoDo, ! FDT_STA_ID %in% lakesPolysAUjoin$FDT_STA_ID)

nrow(distinctSites_AU) + nrow(distinctSites_AUtoDo) == nrow(distinctSites_AUall)


rm(lakesPolysAUjoin);rm(estuaryPolysAUjoin)
```



### Spatially Join AU Lines

Now on to the more computationally heavy AU line snapping methods. First we will try to attach riverine AUs, and where stations remain we will try the estuarine lines AU snap.

##### Riverine Lines AU

Buffer all sites that don't fall into a polygon layer. The output will add a field called `Buffer Distance` to the distinctSites_AU to indicate distance required for snapping. This does not get transported to the data of record, but it is useful to keep for now for QA purposes. If more than one segment is found within a set buffer distance, that many rows will be attached to the WQStable with the identifying station name. It is up to the QA tool to help the user determine which of these AU's are correct and drop the other records.

Removes any riverine sites from the data frame of unique sites that need AU information.



```{r riverine methods AU}
source('snappingFunctions/snapPointToStreamNetwork.R')

riverineAU <- st_read('C:/HardDriveBackup/GIS/Assessment/2022IR_final/2022IR_GISData/va_aus_riverine.shp') %>%
     st_transform(102003)
  # 2/3/21 have to manually feed character proj4string bc doesn't recognize albers epsg 
  # st_transform('+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=37.5 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m no_defs')

  # Even the Edzer hack didn't work on riverine layers so exported draft riverine layer to shapefile for now. Hopefully final dataset comes as decent .gdb
  #st_read('C:/HardDriveBackup/GIS/Assessment/2020IR_draft/va_20ir_aus.gdb', layer = 'va_2020_aus_riverine' , 
              #            fid_column_name = "OBJECTID") %>%
  #st_transform(102003) %>% # forcing to albers from start bc such a huge layer   
  #st_cast("MULTILINESTRING") # special step for weird WKB error reading in geodatabase, never encountered before, fix from: https://github.com/r-spatial/sf/issues/427


snapTable <- snapAndOrganize(distinctSites_AUtoDo, 'FDT_STA_ID', riverineAU, 
                             bufferDistances = seq(20,80,by=20),  # buffering by 20m from 20 - 80 meters
                             tibble(StationID = character(), ID305B = character(), `Buffer Distance` = character()),
                             "ID305B")

#snapTable <- readRDS('preprocessingWorkflow/snapTable.RDS')

snapTable <- snapTable %>%
  left_join(distinctSites_AUtoDo, by = c('StationID' = 'FDT_STA_ID')) %>% # get station information
  rename('FDT_STA_ID' = 'StationID') %>%
  mutate(ID305B_1 = ID305B) %>%
  dplyr::select(names(distinctSites_AU), `Buffer Distance`) %>%
  group_by(FDT_STA_ID) %>%
  mutate(n = n()) %>%
  ungroup()
  
  
rm(riverineAU)
```

Add these sites to the sites with AU information.

```{r add to AU table}
distinctSites_AU <- bind_rows(distinctSites_AU , snapTable )
```

Remove stations that attached to riverine segments from the 'to do' list.

```{r remove riverine snapped AU sites}
distinctSites_AUtoDo <- filter(distinctSites_AUtoDo, ! FDT_STA_ID %in% snapTable$FDT_STA_ID)
```


We don't have estuarine lines AU information, so the sites that don't connect to any AU's at the max buffer distance will have to be sorted out by the assessors.

```{r}
distinctSites_AU <- distinctSites_AU %>%
  group_by(FDT_STA_ID) %>%
  mutate(n=n())
```


Make sure all stations from original distinct station list have some sort of record (blank or populated) in the distinctSites_AU dataset.

```{r double check no one lost AU}

# check everyone dealt with
nrow(distinctSites_AU) + nrow(distinctSites_AUtoDo) == nrow(distinctSites_AUall)

distinctSites_AU$FDT_STA_ID[!(distinctSites_sf$FDT_STA_ID %in% unique(distinctSites_AU$FDT_STA_ID))]

if(nrow(distinctSites_AUtoDo) == 0){rm(distinctSites_AUtoDo)}

#View(filter(distinctSites_sf, FDT_STA_ID %in% distinctSites_AU$FDT_STA_ID[!(distinctSites_sf$FDT_STA_ID %in% unique(distinctSites_AU$FDT_STA_ID))]))
```

And that none of the missing sites (sites that fall outside of the assessmentLayer) are not missing from the AU snapping process.

```{r missingSites got WQS}
missingSites$FDT_STA_ID[!(missingSites$FDT_STA_ID %in% unique(distinctSites_AU$FDT_STA_ID))]
```

Make sure buffer distances save right

```{r}
unique(distinctSites_AU$`Buffer Distance`)

distinctSites_AU$`Buffer Distance` <- as.character(distinctSites_AU$`Buffer Distance`)
```



Save your work!


```{r sites ready for app review}
write.csv(distinctSites_AU , 'data/preAnalyzedAUdata.csv', row.names = F)
```


